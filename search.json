[{"path":"/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"GNU General Public License","title":"GNU General Public License","text":"Version 3, 29 June 2007Copyright © 2007 Free Software Foundation, Inc. <http://fsf.org/> Everyone permitted copy distribute verbatim copies license document, changing allowed.","code":""},{"path":"/LICENSE.html","id":"preamble","dir":"","previous_headings":"","what":"Preamble","title":"GNU General Public License","text":"GNU General Public License free, copyleft license software kinds works. licenses software practical works designed take away freedom share change works. contrast, GNU General Public License intended guarantee freedom share change versions program–make sure remains free software users. , Free Software Foundation, use GNU General Public License software; applies also work released way authors. can apply programs, . speak free software, referring freedom, price. General Public Licenses designed make sure freedom distribute copies free software (charge wish), receive source code can get want , can change software use pieces new free programs, know can things. protect rights, need prevent others denying rights asking surrender rights. Therefore, certain responsibilities distribute copies software, modify : responsibilities respect freedom others. example, distribute copies program, whether gratis fee, must pass recipients freedoms received. must make sure , , receive can get source code. must show terms know rights. Developers use GNU GPL protect rights two steps: (1) assert copyright software, (2) offer License giving legal permission copy, distribute /modify . developers’ authors’ protection, GPL clearly explains warranty free software. users’ authors’ sake, GPL requires modified versions marked changed, problems attributed erroneously authors previous versions. devices designed deny users access install run modified versions software inside , although manufacturer can . fundamentally incompatible aim protecting users’ freedom change software. systematic pattern abuse occurs area products individuals use, precisely unacceptable. Therefore, designed version GPL prohibit practice products. problems arise substantially domains, stand ready extend provision domains future versions GPL, needed protect freedom users. Finally, every program threatened constantly software patents. States allow patents restrict development use software general-purpose computers, , wish avoid special danger patents applied free program make effectively proprietary. prevent , GPL assures patents used render program non-free. precise terms conditions copying, distribution modification follow.","code":""},{"path":[]},{"path":"/LICENSE.html","id":"id_0-definitions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"0. Definitions","title":"GNU General Public License","text":"“License” refers version 3 GNU General Public License. “Copyright” also means copyright-like laws apply kinds works, semiconductor masks. “Program” refers copyrightable work licensed License. licensee addressed “”. “Licensees” “recipients” may individuals organizations. “modify” work means copy adapt part work fashion requiring copyright permission, making exact copy. resulting work called “modified version” earlier work work “based ” earlier work. “covered work” means either unmodified Program work based Program. “propagate” work means anything , without permission, make directly secondarily liable infringement applicable copyright law, except executing computer modifying private copy. Propagation includes copying, distribution (without modification), making available public, countries activities well. “convey” work means kind propagation enables parties make receive copies. Mere interaction user computer network, transfer copy, conveying. interactive user interface displays “Appropriate Legal Notices” extent includes convenient prominently visible feature (1) displays appropriate copyright notice, (2) tells user warranty work (except extent warranties provided), licensees may convey work License, view copy License. interface presents list user commands options, menu, prominent item list meets criterion.","code":""},{"path":"/LICENSE.html","id":"id_1-source-code","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"1. Source Code","title":"GNU General Public License","text":"“source code” work means preferred form work making modifications . “Object code” means non-source form work. “Standard Interface” means interface either official standard defined recognized standards body, , case interfaces specified particular programming language, one widely used among developers working language. “System Libraries” executable work include anything, work whole, () included normal form packaging Major Component, part Major Component, (b) serves enable use work Major Component, implement Standard Interface implementation available public source code form. “Major Component”, context, means major essential component (kernel, window system, ) specific operating system () executable work runs, compiler used produce work, object code interpreter used run . “Corresponding Source” work object code form means source code needed generate, install, (executable work) run object code modify work, including scripts control activities. However, include work’s System Libraries, general-purpose tools generally available free programs used unmodified performing activities part work. example, Corresponding Source includes interface definition files associated source files work, source code shared libraries dynamically linked subprograms work specifically designed require, intimate data communication control flow subprograms parts work. Corresponding Source need include anything users can regenerate automatically parts Corresponding Source. Corresponding Source work source code form work.","code":""},{"path":"/LICENSE.html","id":"id_2-basic-permissions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"2. Basic Permissions","title":"GNU General Public License","text":"rights granted License granted term copyright Program, irrevocable provided stated conditions met. License explicitly affirms unlimited permission run unmodified Program. output running covered work covered License output, given content, constitutes covered work. License acknowledges rights fair use equivalent, provided copyright law. may make, run propagate covered works convey, without conditions long license otherwise remains force. may convey covered works others sole purpose make modifications exclusively , provide facilities running works, provided comply terms License conveying material control copyright. thus making running covered works must exclusively behalf, direction control, terms prohibit making copies copyrighted material outside relationship . Conveying circumstances permitted solely conditions stated . Sublicensing allowed; section 10 makes unnecessary.","code":""},{"path":"/LICENSE.html","id":"id_3-protecting-users-legal-rights-from-anti-circumvention-law","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"3. Protecting Users’ Legal Rights From Anti-Circumvention Law","title":"GNU General Public License","text":"covered work shall deemed part effective technological measure applicable law fulfilling obligations article 11 WIPO copyright treaty adopted 20 December 1996, similar laws prohibiting restricting circumvention measures. convey covered work, waive legal power forbid circumvention technological measures extent circumvention effected exercising rights License respect covered work, disclaim intention limit operation modification work means enforcing, work’s users, third parties’ legal rights forbid circumvention technological measures.","code":""},{"path":"/LICENSE.html","id":"id_4-conveying-verbatim-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"4. Conveying Verbatim Copies","title":"GNU General Public License","text":"may convey verbatim copies Program’s source code receive , medium, provided conspicuously appropriately publish copy appropriate copyright notice; keep intact notices stating License non-permissive terms added accord section 7 apply code; keep intact notices absence warranty; give recipients copy License along Program. may charge price price copy convey, may offer support warranty protection fee.","code":""},{"path":"/LICENSE.html","id":"id_5-conveying-modified-source-versions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"5. Conveying Modified Source Versions","title":"GNU General Public License","text":"may convey work based Program, modifications produce Program, form source code terms section 4, provided also meet conditions: ) work must carry prominent notices stating modified , giving relevant date. b) work must carry prominent notices stating released License conditions added section 7. requirement modifies requirement section 4 “keep intact notices”. c) must license entire work, whole, License anyone comes possession copy. License therefore apply, along applicable section 7 additional terms, whole work, parts, regardless packaged. License gives permission license work way, invalidate permission separately received . d) work interactive user interfaces, must display Appropriate Legal Notices; however, Program interactive interfaces display Appropriate Legal Notices, work need make . compilation covered work separate independent works, nature extensions covered work, combined form larger program, volume storage distribution medium, called “aggregate” compilation resulting copyright used limit access legal rights compilation’s users beyond individual works permit. Inclusion covered work aggregate cause License apply parts aggregate.","code":""},{"path":"/LICENSE.html","id":"id_6-conveying-non-source-forms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"6. Conveying Non-Source Forms","title":"GNU General Public License","text":"may convey covered work object code form terms sections 4 5, provided also convey machine-readable Corresponding Source terms License, one ways: ) Convey object code , embodied , physical product (including physical distribution medium), accompanied Corresponding Source fixed durable physical medium customarily used software interchange. b) Convey object code , embodied , physical product (including physical distribution medium), accompanied written offer, valid least three years valid long offer spare parts customer support product model, give anyone possesses object code either (1) copy Corresponding Source software product covered License, durable physical medium customarily used software interchange, price reasonable cost physically performing conveying source, (2) access copy Corresponding Source network server charge. c) Convey individual copies object code copy written offer provide Corresponding Source. alternative allowed occasionally noncommercially, received object code offer, accord subsection 6b. d) Convey object code offering access designated place (gratis charge), offer equivalent access Corresponding Source way place charge. need require recipients copy Corresponding Source along object code. place copy object code network server, Corresponding Source may different server (operated third party) supports equivalent copying facilities, provided maintain clear directions next object code saying find Corresponding Source. Regardless server hosts Corresponding Source, remain obligated ensure available long needed satisfy requirements. e) Convey object code using peer--peer transmission, provided inform peers object code Corresponding Source work offered general public charge subsection 6d. separable portion object code, whose source code excluded Corresponding Source System Library, need included conveying object code work. “User Product” either (1) “consumer product”, means tangible personal property normally used personal, family, household purposes, (2) anything designed sold incorporation dwelling. determining whether product consumer product, doubtful cases shall resolved favor coverage. particular product received particular user, “normally used” refers typical common use class product, regardless status particular user way particular user actually uses, expects expected use, product. product consumer product regardless whether product substantial commercial, industrial non-consumer uses, unless uses represent significant mode use product. “Installation Information” User Product means methods, procedures, authorization keys, information required install execute modified versions covered work User Product modified version Corresponding Source. information must suffice ensure continued functioning modified object code case prevented interfered solely modification made. convey object code work section , , specifically use , User Product, conveying occurs part transaction right possession use User Product transferred recipient perpetuity fixed term (regardless transaction characterized), Corresponding Source conveyed section must accompanied Installation Information. requirement apply neither third party retains ability install modified object code User Product (example, work installed ROM). requirement provide Installation Information include requirement continue provide support service, warranty, updates work modified installed recipient, User Product modified installed. Access network may denied modification materially adversely affects operation network violates rules protocols communication across network. Corresponding Source conveyed, Installation Information provided, accord section must format publicly documented (implementation available public source code form), must require special password key unpacking, reading copying.","code":""},{"path":"/LICENSE.html","id":"id_7-additional-terms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"7. Additional Terms","title":"GNU General Public License","text":"“Additional permissions” terms supplement terms License making exceptions one conditions. Additional permissions applicable entire Program shall treated though included License, extent valid applicable law. additional permissions apply part Program, part may used separately permissions, entire Program remains governed License without regard additional permissions. convey copy covered work, may option remove additional permissions copy, part . (Additional permissions may written require removal certain cases modify work.) may place additional permissions material, added covered work, can give appropriate copyright permission. Notwithstanding provision License, material add covered work, may (authorized copyright holders material) supplement terms License terms: ) Disclaiming warranty limiting liability differently terms sections 15 16 License; b) Requiring preservation specified reasonable legal notices author attributions material Appropriate Legal Notices displayed works containing ; c) Prohibiting misrepresentation origin material, requiring modified versions material marked reasonable ways different original version; d) Limiting use publicity purposes names licensors authors material; e) Declining grant rights trademark law use trade names, trademarks, service marks; f) Requiring indemnification licensors authors material anyone conveys material (modified versions ) contractual assumptions liability recipient, liability contractual assumptions directly impose licensors authors. non-permissive additional terms considered “restrictions” within meaning section 10. Program received , part , contains notice stating governed License along term restriction, may remove term. license document contains restriction permits relicensing conveying License, may add covered work material governed terms license document, provided restriction survive relicensing conveying. add terms covered work accord section, must place, relevant source files, statement additional terms apply files, notice indicating find applicable terms. Additional terms, permissive non-permissive, may stated form separately written license, stated exceptions; requirements apply either way.","code":""},{"path":"/LICENSE.html","id":"id_8-termination","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"8. Termination","title":"GNU General Public License","text":"may propagate modify covered work except expressly provided License. attempt otherwise propagate modify void, automatically terminate rights License (including patent licenses granted third paragraph section 11). However, cease violation License, license particular copyright holder reinstated () provisionally, unless copyright holder explicitly finally terminates license, (b) permanently, copyright holder fails notify violation reasonable means prior 60 days cessation. Moreover, license particular copyright holder reinstated permanently copyright holder notifies violation reasonable means, first time received notice violation License (work) copyright holder, cure violation prior 30 days receipt notice. Termination rights section terminate licenses parties received copies rights License. rights terminated permanently reinstated, qualify receive new licenses material section 10.","code":""},{"path":"/LICENSE.html","id":"id_9-acceptance-not-required-for-having-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"9. Acceptance Not Required for Having Copies","title":"GNU General Public License","text":"required accept License order receive run copy Program. Ancillary propagation covered work occurring solely consequence using peer--peer transmission receive copy likewise require acceptance. However, nothing License grants permission propagate modify covered work. actions infringe copyright accept License. Therefore, modifying propagating covered work, indicate acceptance License .","code":""},{"path":"/LICENSE.html","id":"id_10-automatic-licensing-of-downstream-recipients","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"10. Automatic Licensing of Downstream Recipients","title":"GNU General Public License","text":"time convey covered work, recipient automatically receives license original licensors, run, modify propagate work, subject License. responsible enforcing compliance third parties License. “entity transaction” transaction transferring control organization, substantially assets one, subdividing organization, merging organizations. propagation covered work results entity transaction, party transaction receives copy work also receives whatever licenses work party’s predecessor interest give previous paragraph, plus right possession Corresponding Source work predecessor interest, predecessor can get reasonable efforts. may impose restrictions exercise rights granted affirmed License. example, may impose license fee, royalty, charge exercise rights granted License, may initiate litigation (including cross-claim counterclaim lawsuit) alleging patent claim infringed making, using, selling, offering sale, importing Program portion .","code":""},{"path":"/LICENSE.html","id":"id_11-patents","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"11. Patents","title":"GNU General Public License","text":"“contributor” copyright holder authorizes use License Program work Program based. work thus licensed called contributor’s “contributor version”. contributor’s “essential patent claims” patent claims owned controlled contributor, whether already acquired hereafter acquired, infringed manner, permitted License, making, using, selling contributor version, include claims infringed consequence modification contributor version. purposes definition, “control” includes right grant patent sublicenses manner consistent requirements License. contributor grants non-exclusive, worldwide, royalty-free patent license contributor’s essential patent claims, make, use, sell, offer sale, import otherwise run, modify propagate contents contributor version. following three paragraphs, “patent license” express agreement commitment, however denominated, enforce patent (express permission practice patent covenant sue patent infringement). “grant” patent license party means make agreement commitment enforce patent party. convey covered work, knowingly relying patent license, Corresponding Source work available anyone copy, free charge terms License, publicly available network server readily accessible means, must either (1) cause Corresponding Source available, (2) arrange deprive benefit patent license particular work, (3) arrange, manner consistent requirements License, extend patent license downstream recipients. “Knowingly relying” means actual knowledge , patent license, conveying covered work country, recipient’s use covered work country, infringe one identifiable patents country reason believe valid. , pursuant connection single transaction arrangement, convey, propagate procuring conveyance , covered work, grant patent license parties receiving covered work authorizing use, propagate, modify convey specific copy covered work, patent license grant automatically extended recipients covered work works based . patent license “discriminatory” include within scope coverage, prohibits exercise , conditioned non-exercise one rights specifically granted License. may convey covered work party arrangement third party business distributing software, make payment third party based extent activity conveying work, third party grants, parties receive covered work , discriminatory patent license () connection copies covered work conveyed (copies made copies), (b) primarily connection specific products compilations contain covered work, unless entered arrangement, patent license granted, prior 28 March 2007. Nothing License shall construed excluding limiting implied license defenses infringement may otherwise available applicable patent law.","code":""},{"path":"/LICENSE.html","id":"id_12-no-surrender-of-others-freedom","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"12. No Surrender of Others’ Freedom","title":"GNU General Public License","text":"conditions imposed (whether court order, agreement otherwise) contradict conditions License, excuse conditions License. convey covered work satisfy simultaneously obligations License pertinent obligations, consequence may convey . example, agree terms obligate collect royalty conveying convey Program, way satisfy terms License refrain entirely conveying Program.","code":""},{"path":"/LICENSE.html","id":"id_13-use-with-the-gnu-affero-general-public-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"13. Use with the GNU Affero General Public License","title":"GNU General Public License","text":"Notwithstanding provision License, permission link combine covered work work licensed version 3 GNU Affero General Public License single combined work, convey resulting work. terms License continue apply part covered work, special requirements GNU Affero General Public License, section 13, concerning interaction network apply combination .","code":""},{"path":"/LICENSE.html","id":"id_14-revised-versions-of-this-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"14. Revised Versions of this License","title":"GNU General Public License","text":"Free Software Foundation may publish revised /new versions GNU General Public License time time. new versions similar spirit present version, may differ detail address new problems concerns. version given distinguishing version number. Program specifies certain numbered version GNU General Public License “later version” applies , option following terms conditions either numbered version later version published Free Software Foundation. Program specify version number GNU General Public License, may choose version ever published Free Software Foundation. Program specifies proxy can decide future versions GNU General Public License can used, proxy’s public statement acceptance version permanently authorizes choose version Program. Later license versions may give additional different permissions. However, additional obligations imposed author copyright holder result choosing follow later version.","code":""},{"path":"/LICENSE.html","id":"id_15-disclaimer-of-warranty","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"15. Disclaimer of Warranty","title":"GNU General Public License","text":"WARRANTY PROGRAM, EXTENT PERMITTED APPLICABLE LAW. EXCEPT OTHERWISE STATED WRITING COPYRIGHT HOLDERS /PARTIES PROVIDE PROGRAM “” WITHOUT WARRANTY KIND, EITHER EXPRESSED IMPLIED, INCLUDING, LIMITED , IMPLIED WARRANTIES MERCHANTABILITY FITNESS PARTICULAR PURPOSE. ENTIRE RISK QUALITY PERFORMANCE PROGRAM . PROGRAM PROVE DEFECTIVE, ASSUME COST NECESSARY SERVICING, REPAIR CORRECTION.","code":""},{"path":"/LICENSE.html","id":"id_16-limitation-of-liability","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"16. Limitation of Liability","title":"GNU General Public License","text":"EVENT UNLESS REQUIRED APPLICABLE LAW AGREED WRITING COPYRIGHT HOLDER, PARTY MODIFIES /CONVEYS PROGRAM PERMITTED , LIABLE DAMAGES, INCLUDING GENERAL, SPECIAL, INCIDENTAL CONSEQUENTIAL DAMAGES ARISING USE INABILITY USE PROGRAM (INCLUDING LIMITED LOSS DATA DATA RENDERED INACCURATE LOSSES SUSTAINED THIRD PARTIES FAILURE PROGRAM OPERATE PROGRAMS), EVEN HOLDER PARTY ADVISED POSSIBILITY DAMAGES.","code":""},{"path":"/LICENSE.html","id":"id_17-interpretation-of-sections-15-and-16","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"17. Interpretation of Sections 15 and 16","title":"GNU General Public License","text":"disclaimer warranty limitation liability provided given local legal effect according terms, reviewing courts shall apply local law closely approximates absolute waiver civil liability connection Program, unless warranty assumption liability accompanies copy Program return fee. END TERMS CONDITIONS","code":""},{"path":"/articles/FourDModelleR.html","id":"installation","dir":"Articles","previous_headings":"","what":"Installation","title":"Getting started","text":"get ModelleR installed first need make sure recent version R installed. easiest way install RStudio. Next clone repository move directory. Next need install INLA, won’t installed default install FourDModellR package. open R console RStudio console run Next can install FourDModellR package local directory. install FourDModellR package dependencies.","code":"git clone https://github.com/4DModeller/fdmr.git cd fdmr # install.packages(\"INLA\", repos = c(getOption(\"repos\"), INLA = \"https://inla.r-inla-download.org/R/stable\"), dep = TRUE) # install.packages(\".\", repos = NULL, type = \"source\")"},{"path":"/articles/FourDModelleR.html","id":"configuration-file","dir":"Articles","previous_headings":"","what":"Configuration file","title":"Getting started","text":"’ve installed package need run fdmr::create_config() create configuration file fdmr knows store intermediate files caches. can run configuration setup just pressing enter, change default paths entering required path prompted. ’re finished configuration file written ~/.config/fdmr/config.conf. ’re now ready move onto one tutorials.","code":"# fdmr::create_config()"},{"path":"/articles/covid.html","id":"modelling-covid-19-infection-across-england","dir":"Articles","previous_headings":"","what":"Modelling COVID-19 infection across England","title":"COVID-19","text":"tutorial ’ll cover work part study fitting Bayesian spatio-temporal model predict COVID-19 infection rate across England. ’ll cover …","code":""},{"path":"/articles/covid.html","id":"study-aim-and-data-description","dir":"Articles","previous_headings":"","what":"Study aim and data description","title":"COVID-19","text":"COVID-19 pandemic profound impact global health economies, spread evolution virus becoming major concern health authorities policymakers. study, aim investigate spread evolution COVID-19 occurrences across England. aim study two-fold: one hand, fitting Bayesian spatio-temporal model predict COVID-19 infection rate across mainland England space time; hand, investigating impacts socioeconomic, demographic environmental factors COVID-19 infection. first thing load packages used COVID-19 case study. study region mainland England, partitioned 6789 neighbourhoods Middle Layer Super Output Area (MSOA) scale. shapefile study region shape SpatialPolygonsDataFrame, used map data. stores location, shape attributes geographic features neighbourhoods. first need retrieve data fdmr example data store unpack ’ll use retrieve_tutorial_data . NOTE: Download tar file URL sent pass path file function. process streamlined know can host publicly. Next ’ll use load_tutorial_data function load spatial data want. Now make map study region COVID-19 data related covariate information included tutorial data package. ’ll load data using process used . first 6 rows data set can viewed using following code response variable study weekly reported number COVID-19 cases 6789 neighbourhoods main England period 2020-03-07 2022-03-26. week defined range Saturday Friday. Variable date indicates start date observation week variable week indicates week number data collected . Therefore, time frame study week beginning March 7, 2020 week beginning March 26, 2022 inclusive, spans 108 weeks. Variables LONG LAT indicate longitude latitude neighbourhood. plot weekly number reported COVID-19 cases spanning 108 weeks: number studies linked different socioeconomic, demographic environmental factors explain variation COVID-19 infection rates across space (Sun, Hu, Xie (2021), Choi et al. (2021), Akinwumiju et al. (2022), Al Kindi et al. (2021), Kim et al. (2021), Wang et al. (2020), Berg, Present, Richardson (2021)). Based past studies data availability, selected set potential socioeconomic, demographic environmental variables model. covariate data collected MSOA England, can accessed https://www.nomisweb.co.uk/census/2021/bulk https://uk-air.defra.gov.uk/data/pcm-data. also considered two covariates related health care possible inclusion model, existing studies shown may affect infection rates COVID-19 (Harris Brunsdon (2021), Lee et al. (2022)). Table 1 provides full descriptions variables. Table1: Descriptions covariates study","code":"library(spdep) library(rgdal) library(gridExtra) library(INLA) library(inlabru) library(dplyr) library(raster) library(leaflet) library(scales) library(ggplot2) library(fdmr) tarpath <- \"/example/path/covid19_example.tar.bz2\" fdmr::retrieve_tutorial_data(tarball_path = tarpath) sp_data <- fdmr::load_tutorial_data(filename = \"spatial_data.rds\") sp_data@data$mapp <- 0 domain <- sp_data@data$mapp  fdmr::plot_map(data = sp_data, domain = domain, add_scale_bar = TRUE, polygon_fill_opacity = 0.5, fill_colour_weight = 0.6) covid19_data <- fdmr::load_tutorial_data(filename = \"covid19_data.rds\") utils::head(covid19_data) breaks_vec <- c(seq(as.Date(\"2020-03-07\"),   as.Date(\"2022-03-26\"),   by = \"3 week\" ), as.Date(\"2022-03-26\"))  cases_week <- dplyr::group_by(covid19_data, date) %>% dplyr::summarize(cases = sum(cases))  fdmr::plot_barchart(data = cases_week, x = cases_week$date, y = cases_week$cases, breaks = breaks_vec, x_label = \"Date\", y_label = \"Number of cases\")"},{"path":"/articles/covid.html","id":"model-specification","dir":"Articles","previous_headings":"","what":"Model specification","title":"COVID-19","text":"use Bayesian hierarchical model predict spatio-temporal COVID-19 infection rate neighbourhood level England. Let \\(Y_{}\\) denotes weekly number reported COVID cases neighbourhood \\(=1,\\ldots, n(=6789)\\) week \\(t=1,\\ldots, T(=108)\\) \\(N_{}\\) denotes (official) estimated population living neighbourhood \\(\\) week \\(t\\). Note data population size neighbourhood obtained 2021 census change time, .e., \\(N_{i1}=N_{i2}=\\ldots = N_{,108}\\) \\(\\). \\(Y_{}\\) assumed Poisson distribution parameters (\\(N_{}\\), \\(\\theta_{}\\)), \\(\\theta_{}\\) true unobserved COVID-19 infection rate / risk neighbourhood \\(\\) week \\(t\\). follow standard path modelling \\(\\theta_{}\\) log link Poisson start model linear predictor decomposes additively set covariates Gaussian latent process characterizing infection disease covariate effects accounted . proposed model given \\[\\begin{align} \\nonumber  Y_{}\\vert N_{}, \\theta_{} &\\sim \\text{Poisson}(N_{}\\theta_{}),\\ \\  =1,\\ldots,n;\\ \\  t=1,\\ldots,T,\\\\ log(\\theta_{} )&=\\boldsymbol{x_{}^{\\top}}\\boldsymbol{\\beta}+S(,t). \\end{align}\\] vector covariates (needed) given \\(\\boldsymbol{x_{}}\\) neighbourhood \\(\\) time period \\(t\\). \\(\\boldsymbol\\beta\\) vector regression parameters. \\(S(,t)\\) spatio-temporal random effect location \\(\\) time \\(t\\), modelled \\[S(,t)=\\alpha \\times S(,t-1)+\\omega(,t).\\] \\(S(,t)\\) follows stationary distribution first-order autoregressive process (AR(1)) \\(\\alpha\\) temporal dependence parameter takes value interval [-1,1], \\(\\alpha=1\\) indicating strong temporal dependence (first-order random walk), \\(\\alpha=0\\) corresponds independence across time. \\(\\omega(,t)\\) spatial random effect assumed arise multivariate normal distribution. \\(\\omega(,t)\\) follows zero-mean Gaussian field assumed temporally independent spatially dependent time period Matérn covariance function given \\[\\text{Cov}(\\omega(,t), \\omega(j,t))=\\frac{\\sigma^2}{2^{\\nu-1}\\Gamma(\\nu)}(\\kappa||-j||)^{\\nu}K_{\\nu}(\\kappa||-j||),\\] \\(K_{\\nu}(\\cdot)\\) modified Bessel function second kind, \\(\\Gamma(\\nu)\\) Gamma function. Matérn covariance function three hyperparameters: \\(\\sigma^2\\) controls marginal variance process S(,t). \\(\\kappa\\) controls spatial correlation range, can defined \\(\\rho=\\sqrt{8\\nu}/\\kappa\\). \\(\\nu\\) controls smoothness, higher values leads processes smoother. model implemented INLA-SPDE approach R programming. steps needed fitting model: Create triangulated mesh study region Build SPDE model based mesh set priors spatial parameters Define process evolves time set prior temporal parameter Define model formula","code":""},{"path":"/articles/covid.html","id":"mesh-construction","dir":"Articles","previous_headings":"","what":"Mesh construction","title":"COVID-19","text":"implement SPDE approach, necessary discretize space creating triangulated mesh establishes set artificial neighbors across study region (.e., mainland England). allows calculation spatial autocorrelation observations. construction mesh impact model inferences predictions. Therefore, crucial develop good mesh ensure results overly sensitive mesh. Although construction mesh varies depending case study, guidelines available produce optimal mesh. vignette construct two-dimensional mesh using INLA::inla.mesh.2d() function. locations neighbourhoods passed function argument “loc” initial mesh nodes, arguments function tuned adjust shape resolution mesh necessary. Argument “max.edge” determines maximum permitted length triangle (lower values max.edge result higher mesh resolution). argument can take either scalar value, controls triangle edge lengths inner domain, length-two vector controls edge lengths inner domain outer extension avoid boundary effect. Although standard setting correct value max.edge, value max.edge close spatial range (.e., low resolution) complicate process fitting smooth SPDE. Specifically, Gaussian random field approximation might deviate strongly desired Matern structure, marginal variance vary domain rather constant, violates assumption stationary Gaussian process. Conversely, max.edge value small compared spatial range (.e., high resolution), mesh contain excessive number vertices, resulting computationally intensive fitting process may necessarily yield improved results. number research studies suggested max.edge value inner domain 1/3 1/10 times smaller spatial range (note spatial range parameter INLA GMRF distance correlation drops 0.13). Since spatial range known model fitted, initial guess needs made 1/5 spatial domain often used practice. , define argument “max.edge” outer extension triangle density two times lower inner domain (.e., twice triangle length outer extension edges), original spatial domain extended without increasing much computational burden. Lindgren Rue (2015) suggested extending domain interest distance least equal spatial range avoid boundary effect. function argument “offset” specifies size inner outer extensions around data locations. study, expand inner domain 1/4 spatial range initially assumed, outer domain amount spatial range. addition, also use parameter “cutoff” avoid building many small triangles close input locations. defines minimum allowed distance observation points. Points distance less cutoff value considered single mesh vertex. choose cutoff value equal 1/7 max.edge value inner domain. mesh study region displayed Figure ??, blue dots represent locations neighbourhoods. use INLA::inla.mesh.2d construct mesh.","code":"initial_range <- diff(range(sp_data@data[, \"LONG\"])) / 5  max_edge <- initial_range / 8  mesh <- INLA::inla.mesh.2d(   loc = sp_data@data[, c(\"LONG\", \"LAT\")],   max.edge = c(1, 2) * max_edge,   offset = c(initial_range / 4, initial_range),   cutoff = max_edge / 7 ) point_data <- sp_data@data[, c(\"LONG\", \"LAT\")]  fdmr::plot_mesh(mesh = mesh, point_data = point_data)"},{"path":"/articles/covid.html","id":"build-the-spde-model-on-the-mesh-and-set-priors-for-the-spatial-parameters","dir":"Articles","previous_headings":"","what":"Build the SPDE model on the mesh and set priors for the spatial parameters","title":"COVID-19","text":"use INLA::inla.spde2.pcmatern() function build SPDE model specify Penalised Complexity (PC) priors parameters Matérn field. PC priors parameters range marginal standard deviation Matérn field specified setting values \\(m_r\\), \\(p_r\\), \\(m_\\sigma\\) \\(p_\\sigma\\) relations P(spatial range<\\(m_r\\))= \\(p_r\\), P(\\(\\sigma>m_\\sigma\\))= \\(p_\\sigma\\). spatial range process distance correlation two values close 0.1. study use prior P(spatial range<0.296)= 0.5 spatial range parameter based exploratory variogram analysis. means probability spatial range smaller 0.296 degrees latitude (equivalent 32.856 kilometers) 0.5. \\(\\sigma\\) controls marginal standard deviation process, specified prior P(\\(\\sigma\\) >1)= 0.01.","code":"prior_range <- initial_range spde <- INLA::inla.spde2.pcmatern(   mesh = mesh,   prior.range = c(prior_range, 0.5),   prior.sigma = c(1, 0.01) )"},{"path":"/articles/covid.html","id":"define-how-the-process-evolves-over-time-and-set-prior-for-the-temporal-parameter","dir":"Articles","previous_headings":"","what":"Define how the process evolves over time and set prior for the temporal parameter","title":"COVID-19","text":"previous step specified time periods spatial locations linked SPDE model. Now assume across time process evolves according first order autoregressive (AR(1)) process. specify PC prior temporal autocorrelation parameter \\(\\alpha \\[-1,1]\\). prior given rhoprior, PC prior P(\\(\\alpha>0\\))=0.9. order fit model, also need define temporal index (must integer starting 1) number discrete time points want model. use function bru() package inlabru fit model. bru expects coordinates data, thus transform covid19_data data set SpatialPointsDataFrame using function coordinates() sp package.","code":"rhoprior <- base::list(theta = list(prior = \"pccor1\", param = c(0, 0.9))) group_index <- covid19_data$week n_groups <- length(unique(covid19_data$week)) sp::coordinates(covid19_data) <- c(\"LONG\", \"LAT\")"},{"path":"/articles/covid.html","id":"define-the-model-formula","dir":"Articles","previous_headings":"","what":"Define the model formula","title":"COVID-19","text":"order fit spatio-temporal model, model formula needs defined, including response left-hand side fixed random effects right-hand side.","code":"formula <- cases ~ 0 + Intercept + IMD +   carebeds.ratio + AandETRUE +   perc.chinese + perc.indian + perc.bangladeshi + perc.pakistani + perc.ba + perc.bc + perc.wb +   age1 + age2 + age3 + age4 +   pm25 + no2 +   f(     main = coordinates,     model = spde,     group = group.index,     ngroup = n_groups,     control.group = list(       model = \"ar1\",       hyper = rhoprior     )   )"},{"path":"/articles/covid.html","id":"fit-the-model","dir":"Articles","previous_headings":"","what":"Fit the model","title":"COVID-19","text":"Finally, fit spatio-temporal model using spde approach AR(1) process calling function bru() package inlabru. NOTE: Since data size quite large memory requirements function call high. recommend running suitable high memory (MUCH ENOUGH?) system. case takes 13 hours complete.","code":"# inlabru.model<-bru(formula, data = COVID19dat, #                    family = \"poisson\", #                    E = COVID19dat$Population, #                    control.family = list(link = \"log\"), #                    control.predictor = list(link = 1), #                    options = list( #                      control.inla = list( #                        reordering = \"metis\", #                        int.strategy = \"eb\"), #                       verbose = TRUE, #                      inla.mode=\"experimental\" #                    ) # )"},{"path":"/articles/covid.html","id":"results","dir":"Articles","previous_headings":"","what":"Results","title":"COVID-19","text":"can inspect results typing summary(inlabru_model),shows parameter estimates fixed random effects. marginal distributions hyperparameters can also obtained follows.","code":"model_summary <- summary(inlabru_model)  model_summary_fixed <- inlabru_model$summary.fixed model_hyperparams <- inlabru_model$marginals.hyperpar"},{"path":"/articles/covid.html","id":"summary-of-the-parameter-estimates-of-the-fixed-effects","dir":"Articles","previous_headings":"9 Results","what":"Summary of the parameter estimates of the fixed effects","title":"COVID-19","text":"model summary provided tutorial data package ’ll load now. NOTE: ’ve run full model don’t need load files . Table 2 reports regression coefficient values, estimated relative risks 95% credible intervals covariate COVID infection model. estimated relative risks 95% credible intervals computed exponentially transforming regression coefficients associated covariates described Table 1. relative risks relate realistic increases covariate, given brackets column 1 table. Table2: Estimated relative risks 95% credible intervals effects covariate COVID-19 infection. table shows clear evidence high level socio-economic deprivation associated increased COVID-19 incidence. increase 10 scores IMD significantly associated 0.5% decreased infection rate. number care home beds per adult also significantly associated COVID infection, neighbourhood increases care home beds ratio 0.01, estimated risk increases around 1.1%. addition, hospital emergency facilities neighbourhood diagnosed positively significantly associated higher infection risk (RR:1.0050, 95% CI: 1.0018-1.0081). Ethnicity appears overall strong association COVID-19 infection. Neighbourhoods greater percentage Chinese population lower risk infections. However, Indian Pakistani ethnic groups found small detrimental effect COVID infection, risk increasing 0.2% percentage Indian Pakistani ethnic groups neighbourhood increases 1%. African population associated decreased risk COVID-19 infection (RR:0.9961, 95% CI: 0.9955-0.9966), whereas Caribbean population statistically higher COVID-19 infections (RR: 1.0084, 95% CI: 1.0073-1.0095). Neighbourhoods greater percentage white British higher risk infections, 1% increase population associated statistically significant 0.4% increase COVID infection. addition, also found infection rate lower neighbourhoods greater population percentages adults age 65 years older (RR: 0.9937, 95% CI: 0.9932–0.9941) neighbourhoods greater percentages adults age 18–29 years (RR: 0.9962, 95% CI: 0.9958–0.9965). contrast, population 30 44 years old population 45 64 years old positively associated risk infections, although result former significant. Finally, elevated \\(\\text{PM}_{2.5}\\) concentrations statistically significantly associated COVID-19 incidence, \\(1 \\ \\mu g m^{-3}\\) increase concentrations associated 0.02% 0.80% increased risk. \\(10 \\ \\mu g m^{-3}\\) increase \\(\\text{}_2\\) concentrations associated positive, insignificant increase relative risk COVID infections (RR: 1.0015, 95% CI: 0.9915, 1.0116).","code":"model_summary <- fdmr::load_tutorial_data(filename = \"model_summary.rds\") model_summary_fixed <- fdmr::load_tutorial_data(filename = \"model_summary_fixed.rds\") model_hyperparams <- fdmr::load_tutorial_data(filename = \"model_hyperparams.rds\")  print(model_summary_fixed[2:nrow(model_summary_fixed), 1:5])"},{"path":"/articles/covid.html","id":"summary-of-the-hyperparameters","dir":"Articles","previous_headings":"9 Results","what":"Summary of the hyperparameters","title":"COVID-19","text":"parameter estimates spatial range parameter, marginal standard deviation latent Gaussian process AR(1) coefficient plot posterior distributions spatial range, standard deviation temporal dependence parameters spatio-temporal random effect, create list named list_marginals containing posterior distributions parameter. list, generate data frame margs, include additional column named ‘parameter’ specifies name corresponding distribution parameter. Spatial range estimated 0.297, AR(1) coefficient estimated 0.839, indicating high level temporal dependence.","code":"model_summary$inla$hyperpar[, 1:5] list_marginals <- list(   \"Spatial range\" = model_hyperparams$`Range for f`,   \"Stdev\" = model_hyperparams$`Stdev for f`,   \"AR(1)\" = model_hyperparams$`GroupRho for f` )  margs <- data.frame(do.call(rbind, list_marginals)) margs$parameter <- rep(names(list_marginals),   times = sapply(list_marginals, nrow) )  margs$parameter <-   factor(margs$parameter, levels = c(\"Spatial range\", \"Stdev\", \"AR(1)\"))  ggplot(margs, aes(x = x, y = y)) +   geom_line() +   facet_wrap(~parameter, scales = \"free\") +   labs(x = \"\", y = \"Density\") +   theme_bw()"},{"path":"/articles/covid.html","id":"temporal-evolution-of-infection-rate-predictions","dir":"Articles","previous_headings":"9 Results","what":"Temporal evolution of infection rate predictions","title":"COVID-19","text":"infection rate predictions model can computed using following codes, saved data frame called “predictions”. date frame “predictions” provided tutorial data package, ’ll load now. Figure ?? displays boxplots predicted infection risk neighbourhoods time. can seen health inequalities COVID infection exist England, large differences estimated disease risks. Figure ?? provides line plot average predicted risk 95% credible intervals week across England. COVID infection risk witnessed series fluctuations study period within range 0.0002 0.05, overall upward temporal trend can observed. initial peak mid-November 2020, infection levels declined rising mid-December 2020, driven emergence Alpha variant peaked early January 2021. week 5 June 2021, infection rate began rising mid-July estimated 0.5%, risk fell remained steady November. However, infection reached highest point week 8 January 2022 2.0%, decreased substantially early March 2022. Finally, Figure ?? shows spatial pattern time averaged risks infection England.","code":"pred.mean <-   inlabru_model$summary.fitted.values$mean[1:nrow(covid19_data)] pred.25 <-   inlabru_model$summary.fitted.values$`0.025quant`[1:nrow(covid19_data)] pred.975 <-   inlabru_model$summary.fitted.values$`0.975quant`[1:nrow(covid19_data)]  predictions <- cbind.data.frame(   covid19_data$date,   covid19_data$week,   \"pred.mean\" = pred.mean,   \"pred.25\" = pred.25,   \"pred.975\" = pred.975 ) predictions <- fdmr::load_tutorial_data(filename = \"predictions.rds\") breaks_vec <- c(seq(as.Date(\"2020-03-07\"),   max(predictions$date),   by = \"3 week\" ), as.Date(\"2022-03-26\"))  fdmr::plot_boxplot(   data = predictions,   x = predictions$date,   y = predictions$pred.mean,   breaks = breaks_vec,   x_label = \"Week of date\",   y_label = \"Infection risk\" ) mean_week <- dplyr::group_by(predictions, date) %>% dplyr::summarize(   mean.prev = mean(pred.mean),   lc = mean(pred.25),   uC = mean(pred.975) )  fdmr::plot_line_average(   data = mean_week,   x = mean_week$date,   y1 = mean_week$mean.prev,   y2 = mean_week$lc,   y3 = mean_week$uC,   breaks = breaks_vec,   x_label = \"Week of date\",   y_label = \"Average risk\",   y_lim = c(0, 0.025) ) average_risk_by_nb <-   dplyr::group_by(predictions, MSOA11CD) %>% dplyr::summarize(ave.risk = mean(pred.mean))  sp_data@data$ave.risk <- average_risk_by_nb$ave.risk  domain <- sp_data@data$ave.risk legend_values <- sp_data@data$ave.risk  fdmr::plot_map(   data = sp_data,   domain = domain,   palette = \"Reds\",   legend_values = legend_values,   legend_title = \"Risk\",   add_scale_bar = TRUE,   polygon_fill_opacity = 0.8,   fill_colour_weight = 1.0 )"},{"path":"/articles/covid.html","id":"evaluate-the-performance-of-prediction","dir":"Articles","previous_headings":"9 Results","what":"Evaluate the performance of prediction","title":"COVID-19","text":"Measuring prediction accuracy model critical aspect evaluating performance, especially context time-series data new information becomes available time. measure prediction accuracy model, moving time series window training validation data considered new data comes throughout pandemic. approach allows model continuously update parameters based latest available data, rather relying static dataset may reflect current situation. started initial training dataset 96 weeks 2020/03/07 2022/01/01, used predict infection risk \\(\\theta_{,t}\\) week \\(t = 97\\) neighbourhoods \\(\\). prediction compared actual observed values determine accuracy model. process repeated subsequent week new data becomes available. accuracy risk prediction measured root mean square error (RMSE), bias coverage probabilities 95% credible intervals corresponding risk estimates. RMSE quantifies average magnitude differences predicted observed values, lower value indicates better prediction performance. \\(\\text{RMSE}_t\\) risk estimates week \\(t\\) neighbourhoods calculated \\[\\text{RMSE}_t=\\sqrt{\\frac{1}{n_t}\\sum_{=1}^{n_t}(\\hat{\\theta}_{}-\\theta_{})^2},\\] \\(\\hat{\\theta}_{}\\) \\(\\theta_{}\\) predicted observed infection risk neighbourhood \\(\\) week \\(t\\) respectively. \\(n_t\\) total number neighbourhoods reported COVID infection data week \\(t\\). Bias measures average difference predicted observed values. bias risk predictions neighbourhoods week \\(t\\) calculated \\[\\text{Bias}_t=\\frac{1}{n_t}\\sum_{=1}^{n_t}(\\hat{\\theta}_{}-\\theta_{}).\\] uncertainty risk predictions can measured coverage probabilities 95% credible intervals. 95% coverage probability (denoted \\(\\text{CP}_t\\)) computed proportion 95% credible intervals \\(\\hat{\\theta}_{}\\) contain observed risk. addition, measure magnitude differences predicted observed values relation observed values, relative root mean square error (R-RMSE) relative bias (R-Bias) also computed. relative root mean square error (\\(\\text{R-RMSE}_t\\)) calculated \\[ \\text{R-RMSE}_t=\\sqrt{\\frac{1}{n_t}\\sum_{=1}^{n_t}\\left(\\frac{\\hat{\\theta}_{}-\\theta_{}}{\\theta_{}}\\right)^2}.\\] relative bias (\\(\\text{R-Bias}_t\\)) calculated \\[ \\text{R-Bias}_t=\\frac{1}{n_t}\\sum_{=1}^{n_t}\\left(\\frac{\\hat{\\theta}_{}-\\theta_{}}{\\theta_{}}\\right). \\] new data comes every week, previous data used training model, \\(\\text{RMSE}_t\\), \\(\\text{R-RMSE}_t\\), \\(\\text{Bias}_t\\), \\(\\text{R-Bias}_t\\) \\(\\text{CP}_t\\) updated using new validation data week \\(t\\). Finally, create time series \\(\\text{RMSE}_t\\), \\(\\text{Bias}_t\\) \\(\\text{CP}_t\\) week \\(t\\) \\((97,\\ldots, 108)\\), summarise progress predictive capabilities model time. values \\(\\text{RMSE}_t\\), \\(\\text{Bias}_t\\), \\(\\text{CP}_t\\), \\(\\text{R-RMSE}_t\\) \\(\\text{R-Bias}_t\\) week \\(t\\) \\((97,\\ldots, 108)\\) stored tutorial data package ’ll load data.frame containing data now. Figure ?? displays time series plots \\(\\text{RMSE}_t\\), \\(\\text{Bias}_t\\) \\(\\text{CP}_t\\). plots suggest model performs well predicting rate COVID-19 infection, RMSE bias values low (close 0) compared range observed risks, coverage probabilities close nominal 0.95 level. Table 3 provides median mean values \\(\\text{RMSE}_t\\), \\(\\text{Bias}_t\\), \\(\\text{CP}_t\\),\\(\\text{R-RMSE}_t\\) \\(\\text{R-Bias}_t\\) \\(t \\(97, . . . , 108)\\). Table3: summary RMSE, bias coverage probabilities.","code":"predmetrics <- fdmr::load_tutorial_data(filename = \"predmetrics.rds\") breaks_vec <- c(seq(min(predmetrics$date),   max(predmetrics$date),   by = \"1 week\" ))  RMSEt <- predmetrics[, c(\"date\", \"RMSEt\")] Biast <- predmetrics[, c(\"date\", \"Biast\")] CPt <- predmetrics[, c(\"date\", \"CPt\")]  g1 <-   fdmr::plot_timeseries(     data = RMSEt,     x = RMSEt$date,     y = RMSEt$RMSEt,     breaks = breaks_vec,     x_label = \"Date\",     y_label = \"RMSE\",     y_lim = c(0, 0.05),     horizontal_y = 0   )  g2 <-   fdmr::plot_timeseries(     data = Biast,     x = Biast$date,     y = Biast$Biast,     breaks = breaks_vec,     x_label = \"Date\",     y_label = \"Bias\",     y_lim = c(-0.01, 0.01),     horizontal_y = 0   )  g3 <-   fdmr::plot_timeseries(     data = CPt,     x = CPt$date,     y = CPt$CPt,     breaks = breaks_vec,     x_label = \"Date\",     y_label = \"Coverage probability\",     y_lim = c(0, 1),     horizontal_y = 0.95   )  gridExtra::grid.arrange(g1, g2, g3, nrow = 1)"},{"path":"/articles/covid.html","id":"discussion","dir":"Articles","previous_headings":"","what":"Discussion","title":"COVID-19","text":"study provides valuable insights spread evolution COVID-19 occurrences MSOA level mainland England March 7, 2020 March 26, 2022. found significant health inequalities COVID infection across England, magnitude inequalities appearing increased time, emphasizes need effective strategies address disparities COVID-19 risk different neighbourhoods. COVID infection risk England fluctuated time overall upward trend, people appear higher risk COVID infection June, July, December, January, likely due June July popular months travelling, outdoor activities social gatherings UK, December January associated holiday celebrations (Christmas New Year’s holiday), family gatherings, travel. events often involve close contact others, can increase risk COVID-19 transmission. Colder drier conditions winter months also make COVID-19 virus transmissible (Mecenas et al. (2020), Wang et al. (2021)). low estimated infection risks small variation first weeks study period probably due limited capacity COVID-19 testing first wave pandemic. Testing available priority groups comprehensive, meaning many infected people diagnosed virus. likely led -reporting confirmed COVID-19 cases, turn led low infection rates small variation. results model shown clear evidence socioeconomic deprivation positively associated increased COVID-19 incidence, aligns findings Oluyomi et al. (2021), Kulu Dorey (2021) virus hit harder areas higher deprivation. likely due individuals living deprived areas limited access healthcare facilities resources, exhibiting higher prevalence underlying health conditions, relying heavily public transportation, contribute higher infection rates. Ethnicity found strong association COVID-19 infection. Chinese African populations found associated lower risk infections, whereas Indian Pakistani ethnic groups, Caribbean population, white British population associated higher risk infections. lower infection rates Chinese might attributed greater emphasis collective responsibility community health, helped encourage adherence public health guidelines prevent spread virus, white British individuals appear less likely adhere recommended COVID-19 related health behaviours compared ethnic groups https://www.iser.essex.ac.uk/blog/2021/06/14/--ethnic-differences--adherence--recommended-health-behaviours-related--covid-19. Neighborhoods greater percentage adults aged 65 older, greater percentage adults aged 18-29, found lower risk infections, populations 30 44 years old, 45 64 years old, positively associated risk infections. findings may explained aspects vaccination rates, lifestyle health status. example, older people among first groups prioritized COVID-19 vaccination UK, leading lower COVID infection rates. Young adults (aged 18-29) may likely comply public health guidelines regulations, social distancing wearing masks, avoid exposure virus. Additionally, younger adults likely jobs allow work home, reducing exposure virus public spaces. Conversely, middle-aged adults (aged 45-64 years old) may likely work essential jobs require interact public, increasing exposure virus. Elevated \\(\\text{PM}_{2.5}\\) \\(\\text{}_2\\) levels also found positively associated COVID-19 infections. Exposure air pollution can cause inflammation damage respiratory system, may increase susceptibility respiratory infections COVID-19 (Fattorini Regoli (2020), Comunian et al. (2020)). Air pollution shown worsen underlying health conditions diabetes, cardiovascular, respiratory diseases, known risk factors severe illness COVID-19 (Semczuk-Kaczmarek et al. (2021)). important note relationship COVID-19 infection risk race, age, air pollution socioeconomic status complex multifactorial. results showed risk factors considered study well-established relationship COVID-19 infections, necessarily causal relationship. research needed understand mechanisms behind associations. Nonetheless, findings suggest targeted interventions specific age race groups may necessary control spread COVID-19 different neighborhoods, reducing air pollution levels important public health intervention mitigating spread COVID-19. analysis highlighted associations COVID-19 infection set socioeconomic, demographic environmental factors. several areas future work expand understanding spread evolution COVID-19. beneficial extend study period include recent data order examine potential changes associations COVID-19 risk various factors identified study. analysis can extended examine impacts potential factors, comorbidities COVID-19 vaccination rates, may contribute spread COVID-19. Including factors future studies provide comprehensive understanding determinants COVID-19 risk. information useful policy makers determining effective strategies controlling spread COVID-19 reducing health inequalities England. Finally, important extend study countries regions examine whether results study generalizable settings. provide valuable insights global spread COVID-19 inform development effective strategies targeted actions preventing spread virus.","code":""},{"path":[]},{"path":[]},{"path":"/articles/data_globalmass_gic.html","id":"setup","dir":"Articles","previous_headings":"Creating meshes","what":"Setup","title":"GlobalMass GIC Data Pre-processing","text":"avoid need run meshing data pre-processing steps time run model, ’ll first create intermediate_data folder globalmass_tutorial folder. ’ll use folder store intermediate data meshes processed data ready feed BHM.","code":"mkdir intermediate_data"},{"path":"/articles/data_globalmass_gic.html","id":"creating-a-mesh","dir":"Articles","previous_headings":"Creating meshes","what":"Creating a mesh","title":"GlobalMass GIC Data Pre-processing","text":"first mesh create one glaciers ice caps (GIC). ’ll use file 0.5degree_grid_shapefile_Ant_GrIS_removed.shp Now ’ve created GIC mesh ’re ready move creating data required BHM.","code":"library(processing)  gic_shapefile_path = processing::get_tutorial_datapath(\"0.5degree_grid_shapefile_Ant_GrIS_removed.shp\")  gic_mesh_path = processing::create_gic_mesh(input_filepath = gic_shapefile_path,  output_folder=\"intermediate_data\")  gic_mesh_path"},{"path":"/articles/data_globalmass_gic.html","id":"pre-process-data","dir":"Articles","previous_headings":"Creating meshes","what":"Pre-process data","title":"GlobalMass GIC Data Pre-processing","text":"data need run BHM glaciers ice caps stored file 0.5deg_cumulative_mass_change_tseries.txt.tar.bz2. compressed file containing large amount information, don’t worry decompressing , processing handle us. pass filename get_tutorial_datapath function handle decompression return useful filepath us. ’ll use process_hugonnet_tiles process data format BHM expects. now components need feed BHM, mesh file intermediate data file.","code":"gic_raw_data = processing::get_tutorial_datapath(\"0.5deg_cumulative_mass_change_tseries.txt.tar.bz2\")  gic_raw_data processed_data_filepath = processing::process_hugonnet_tiles(input_filepath = gic_raw_data, output_folder = \"intermediate_data\")  processed_data_filepath"},{"path":"/articles/data_globalmass_gic.html","id":"running-the-bhm","dir":"Articles","previous_headings":"","what":"Running the BHM","title":"GlobalMass GIC Data Pre-processing","text":"run BHM ’ll use 4D Modeller 4D_stats package. Please move onto Getting started 4D_stats.","code":""},{"path":"/articles/globalmass/globalmass_oceans.html","id":"getting-setup","dir":"Articles > Globalmass","previous_headings":"","what":"Getting setup","title":"GlobalMass - Oceans","text":"Let’s create folder store data created tutorial. can create folder anywhere make create folder called gm_ocean_tutorial 4DModeller directory home folder. -p flag use tells mkdir create directories time.","code":"mkdir -p ~/4DModeller/gm_oceans_tutorial cd ~/4DModeller/gm_oceans_tutorial"},{"path":"/articles/globalmass/globalmass_oceans.html","id":"retrieving-tutorial-data","dir":"Articles > Globalmass","previous_headings":"","what":"Retrieving tutorial data","title":"GlobalMass - Oceans","text":"’ll first start making sure data files required. First visit link received download .tar.bz2 file using web browser. can easily download file using wget NOTE: step made much simpler user ’ve got somewhere store data. next step unpack tutorial data. provide helper function unpack data location FourDModelleR can easily access. First import FourDModelleR call retrieve_tutorial_data function, passing path file downloaded, ~/4DModeller/gm_oceans/tutorial_data.tar.bz2 tutorial data now extracted ~/4DModeller/tutorial_data. can now move processing data run BHM.","code":"wget <url-given> library(FourDModelleR) tarball_path <- \"~/4DModeller/gm_oceans_tutorial/tutorial_data.tar.bz2\" FourDModelleR::retrieve_tutorial_data(tarball_path = tarball_path)"},{"path":"/articles/globalmass/globalmass_oceans.html","id":"creating-meshes","dir":"Articles > Globalmass","previous_headings":"","what":"Creating meshes","title":"GlobalMass - Oceans","text":"create ocean mesh create_ocean_mesh look multiple shapefiles within folder ’ll just give path shapefiles folder. also need tell function output processed data, can folder select gm_oceans_tutorial folder created earler. take look mesh_paths ’ll notice get two filepaths, one mesh another mesh triangles file. ’ll use mesh tutorial, take first element returned list assign variable use later. Now ’ve created ocean mesh ’re ready move creating data required BHM.","code":"shapefile_folder <- \"~/4DModeller/tutorial_data/bhm_ocean/shapefiles\" output_folder <- \"~/4DModeller/gm_oceans_tutorial/output_data\"  mesh_paths <- FourDModelleR::create_ocean_mesh(shapefile_folder = shapefile_folder, output_folder = output_folder) mesh_paths ocean_mesh_filepath <- mesh_paths[1]"},{"path":[]},{"path":"/articles/globalmass/globalmass_oceans.html","id":"argo","dir":"Articles > Globalmass","previous_headings":"Preprocessing data","what":"Argo","title":"GlobalMass - Oceans","text":"data need run BHM glaciers ice caps stored file Argo_MassSpecVol_ts_CMEMS_vector_Downsampled_2000m_2002_2020_flagvalid_dimrename.nc. one files unpacked retrieve_tutorial_data can get path using get_tutorial_datapath. Next ’ll use process_ocean_argo process data format BHM expects.","code":"argo_data_filepath <- FourDModelleR::get_tutorial_datapath(filename = \"Argo_MassSpecVol_ts_CMEMS_vector_Downsampled_2000m_2002_2020_flagvalid_dimrename.nc\") processed_argo_filepath <- FourDModelleR::process_ocean_argo(input_filepath = argo_data_filepath, output_folder = output_folder)"},{"path":"/articles/globalmass/globalmass_oceans.html","id":"altimetry","dir":"Articles > Globalmass","previous_headings":"Preprocessing data","what":"Altimetry","title":"GlobalMass - Oceans","text":"Next process altimetry data using process_altimetry_proc_gmgia. function requires file Alt_ts_CMEMS_ArcticDTU_GMGIA_MeasErr_Filtered_vector_2005_2015.nc tutorial data pack downloaded earlier. get path using get_tutorial_datapath pass path output_folder processing function.","code":"alt_data_filepath <- FourDModelleR::get_tutorial_datapath(filename = \"Alt_ts_CMEMS_ArcticDTU_GMGIA_MeasErr_Filtered_vector_2005_2015_dimrename.nc\")  processed_alt_filepath <- FourDModelleR::process_altimetry_proc_gmgia(input_filepath = alt_data_filepath, output_folder = output_folder)  processed_alt_filepath"},{"path":"/articles/globalmass/globalmass_oceans.html","id":"grace-data","dir":"Articles > Globalmass","previous_headings":"Preprocessing data","what":"GRACE data","title":"GlobalMass - Oceans","text":"also need GRACE data pass BHM, requires preprocessing just retrieve filepath. now components need pass BHM. Ocean mesh Processed Argo data Processed altimetry data GRACE JPL data","code":"grace_jpl_filepath <- FourDModelleR::get_tutorial_datapath(filename = \"EWH_JPL_grace_global_GMGIA_to2015.RData\")  grace_jpl_filepath"},{"path":"/articles/globalmass/globalmass_oceans.html","id":"running-the-bhm","dir":"Articles > Globalmass","previous_headings":"","what":"Running the BHM","title":"GlobalMass - Oceans","text":"NOTE: best present , much filepath handling? run BHM ’ll use run_bhm_ocean function pass data ’ve just created path output folder. also need tell function whether ’re using JPL GSFC data, ’re using JPL. model output screen ’s running completed output summary model parameters. output also written specified output folder filename looks like FourDModelleR_bhm_ocean_run_{timestamp}.log.","code":"model_output <- FourDModelleR::run_bhm_ocean(   mesh_filepath = ocean_mesh_filepath,   grace_data_filepath = grace_jpl_filepath,   altimetry_filepath = processed_alt_filepath,   argo_data_fileapth = processed_argo_filepath,   output_folder = output_folder,   GRACE_dc = \"JPL\" )"},{"path":"/articles/globalmass/globalmass_oceans.html","id":"analysing-the-model-output","dir":"Articles > Globalmass","previous_headings":"","what":"Analysing the model output","title":"GlobalMass - Oceans","text":"analyse model output can use FourDModelleR::create_report_* functions…","code":""},{"path":[]},{"path":"/articles/globalmass.html","id":"setup","dir":"Articles","previous_headings":"Creating meshes","what":"Setup","title":"GlobalMass","text":"avoid need run meshing data pre-processing steps time run model, ’ll first create intermediate_data folder globalmass_tutorial folder. ’ll use folder store intermediate data meshes processed data ready feed BHM.","code":"mkdir intermediate_data"},{"path":"/articles/globalmass.html","id":"creating-a-mesh","dir":"Articles","previous_headings":"Creating meshes","what":"Creating a mesh","title":"GlobalMass","text":"first mesh create one glaciers ice caps (GIC). ’ll use file 0.5degree_grid_shapefile_Ant_GrIS_removed.shp Now ’ve created GIC mesh ’re ready move creating data required BHM.","code":"library(processing)  gic_shapefile_path = processing::get_tutorial_datapath(\"0.5degree_grid_shapefile_Ant_GrIS_removed.shp\")  gic_mesh_path = processing::create_gic_mesh(input_filepath = gic_shapefile_path,  output_folder=\"intermediate_data\")  gic_mesh_path"},{"path":"/articles/globalmass.html","id":"pre-process-data","dir":"Articles","previous_headings":"Creating meshes","what":"Pre-process data","title":"GlobalMass","text":"data need run BHM glaciers ice caps stored file 0.5deg_cumulative_mass_change_tseries.txt.tar.bz2. compressed file containing large amount information, don’t worry decompressing , processing handle us. pass filename get_tutorial_datapath function handle decompression return useful filepath us. ’ll use process_hugonnet_tiles process data format BHM expects. now components need feed BHM, mesh file intermediate data file.","code":"gic_raw_data = processing::get_tutorial_datapath(\"0.5deg_cumulative_mass_change_tseries.txt.tar.bz2\")  gic_raw_data processed_data_filepath = processing::process_hugonnet_tiles(input_filepath = gic_raw_data, output_folder = \"intermediate_data\")  processed_data_filepath"},{"path":"/articles/globalmass.html","id":"running-the-bhm","dir":"Articles","previous_headings":"","what":"Running the BHM","title":"GlobalMass","text":"run BHM ’ll use 4D Modeller stats package.","code":""},{"path":"/articles/hydro.html","id":"setting-up-the-r-environment","dir":"Articles","previous_headings":"","what":"Setting up the R environment","title":"Hydrology","text":"first thing load packages need use tutorial: INLA inlabru installed please go following installation tutorial continuing. COMMENT: maybe just dependencies 4D-Modeller... Next create variables data stored.","code":"library(leaflet) library(jsonlite) library(ggplot2) library(inlabru) library(INLA) # TODO : this should not use rgdal because it is being deprecated library(rgdal) library(raster) library(ncdf4) library(data.table) # TODO : this is probably too specific still norway_polygon_location <- \"~/repos/processing/vignettes/data/Kvilldal_Catch_Boundary.geojson\" streamdata_13 <- \"~/repos/processing/vignettes/data/NVEobservations_s36_13.csv\" streamdata_14 <- \"~/repos/processing/vignettes/data/NVEobservations_s36_14.csv\" era5_location <- \"~/repos/processing/vignettes/data/era5_land_daily.nc\""},{"path":"/articles/hydro.html","id":"kvilldal-dam-area","dir":"Articles","previous_headings":"","what":"Kvilldal dam area","title":"Hydrology","text":"First look location dam making map plotting stream gauges . map can see dam, resevoir, catchment area, two stream gauges. area inside shape water accumulates, area outside boundary water goes elsewhere. important later including ERA5-land precipitation data.","code":"# TODO : here is a good example for the map function i recommend norway_polygon <- rgdal::readOGR(norway_polygon_location) ## Error in ogrListLayers(dsn = dsn): Cannot open data source norway_polygon <- sf::st_as_sf(norway_polygon                                , coords=c(\"longitude\", \"latitude\")                                , crs=\"+proj=utm +zone=32\") ## Error in sf::st_as_sf(norway_polygon, coords = c(\"longitude\", \"latitude\"), : object 'norway_polygon' not found suldalsvatnet_dam <- list(longitude=6.517174, latitude=59.490720, name=\"Suldalsvatnet Dam\") stream_gauge_13 <- list(longitude=6.5395789, latitude=59.5815849, name=\"Stream Gauge 13\") stream_gauge_14 <- list(longitude=6.7897968, latitude=59.7531662, name=\"Stream Gauge 14\")   sfc <- sf::st_transform(norway_polygon, crs = \"+proj=longlat +datum=WGS84\") ## Error in sf::st_transform(norway_polygon, crs = \"+proj=longlat +datum=WGS84\"): object 'norway_polygon' not found leaflet(sfc, height = 500, width = 750) %>%   addTiles() %>%   addPolygons(stroke = FALSE, smoothFactor = 0.3,               fillColor = \"red\",               fillOpacity = 0.5) %>%   addTiles(\"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\",            attribution = 'Tiles &copy; Esri &mdash; Source: Esri, i-cubed, USDA, USGS, AEX, GeoEye, Getmapping, Aerogrid, IGN, IGP, UPR-EGP, and the GIS User Community') %>%   addMarkers(lat=suldalsvatnet_dam$latitude, lng=suldalsvatnet_dam$longitude, popup=suldalsvatnet_dam$name) %>%   addMarkers(lat=stream_gauge_13$latitude, lng=stream_gauge_13$longitude, popup=stream_gauge_13$name) %>%   addMarkers(lat=stream_gauge_14$latitude, lng=stream_gauge_14$longitude, popup=stream_gauge_14$name) ## Error in structure(list(options = options), leafletData = data): object 'sfc' not found"},{"path":"/articles/hydro.html","id":"stream-gauge-data","dir":"Articles","previous_headings":"","what":"Stream gauge data","title":"Hydrology","text":"stream gauge data measured average daily liters/second pass area. data goes back many years, sometimes decades. See : can see, two different stream gauges record different amount stream flow. Therefore likely normalize data using zscore response variable study stream flow measurement. amount water passes stream gauge representative amount water accumulate resevoir. Thus, order understand changes time, important understand physical processes drive water streams feed resevoir.","code":"data_13 <- read.csv(streamdata_13) ## Warning in file(file, \"rt\"): cannot open file ## '/Users/gar/repos/processing/vignettes/data/NVEobservations_s36_13.csv': No ## such file or directory ## Error in file(file, \"rt\"): cannot open the connection data_13$date <- as.Date(data_13$time) ## Error in as.Date(data_13$time): object 'data_13' not found data_13 <- subset(data_13, date >= \"2021-10-01\" & date <= \"2021-10-31\") ## Error in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'subset': object 'data_13' not found row.names(data_13) <- NULL ## Error in row.names(data_13) <- NULL: object 'data_13' not found data_13$time_index <- seq(1, 31, 1) ## Error in data_13$time_index <- seq(1, 31, 1): object 'data_13' not found data_14 <- read.csv(streamdata_14) ## Warning in file(file, \"rt\"): cannot open file ## '/Users/gar/repos/processing/vignettes/data/NVEobservations_s36_14.csv': No ## such file or directory ## Error in file(file, \"rt\"): cannot open the connection data_14$date <- as.Date(data_14$time) ## Error in as.Date(data_14$time): object 'data_14' not found data_14 <- subset(data_14, date >= \"2021-10-01\" & date <= \"2021-10-31\") ## Error in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'subset': object 'data_14' not found row.names(data_14) <- NULL ## Error in row.names(data_14) <- NULL: object 'data_14' not found data_14$time_index <- seq(1, 31, 1) ## Error in data_14$time_index <- seq(1, 31, 1): object 'data_14' not found # Create a line plot using ggplot2 ggplot(data_13, aes(x = as.Date(time), y = value)) +   geom_line(color = \"violet\") +   labs(x = \"Time\", y = \"Stream Flow : Daily Average (Liter/s)\") ## Error in ggplot(data_13, aes(x = as.Date(time), y = value)): object 'data_13' not found ggplot(data_14, aes(x = as.Date(time), y = value)) +   geom_line(color = \"limegreen\") +   labs(x = \"Time\", y = \"Stream Flow : Daily Average (Liter/s)\") ## Error in ggplot(data_14, aes(x = as.Date(time), y = value)): object 'data_14' not found # we rescale the data with the zscore since hte values are so # different between the two stream gauges data_13$value <- scale(data_13$value) ## Error in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'scale': object 'data_13' not found data_14$value <- scale(data_14$value) ## Error in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'scale': object 'data_14' not found"},{"path":"/articles/hydro.html","id":"era5-land-data","dir":"Articles","previous_headings":"","what":"ERA5-land data","title":"Hydrology","text":"ERA5-land data includes many variables. use daily precipitation data. ERA5-land precipitation comes reanalysis climate model. ERA5-land data gridded however need data assigned catchment shapes. Create data frame rsdf containing precipitation data need figure ERA5 precipitation pixels closest stream gauge can match data regression. First need calculate distance pixel stream gauge, use minimum distance choose stream gauge. first create function converts latlong utm coordinate reference system Now can match pixel precipation data nearest stream gauge. Now create new SpatialPointDataFrame inla_data model fitting. data frame contains response predictor data spatial location time point.","code":"# TODO : mapping function here too era5_precip <- stack(era5_location) ## Error in R_nc4_open: No such file or directory ## Error in ncdf4::nc_open(filename, readunlim = FALSE, suppress_dimvals = TRUE): Error in nc_open trying to open file ~/repos/processing/vignettes/data/era5_land_daily.nc (return_on_error= FALSE ) era5_precip %>% values() %>% hist(main=\"Total Precipitation\", col=\"violet\") ## Error in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'hist': error in evaluating the argument 'x' in selecting a method for function 'values': object 'era5_precip' not found sr <- \"+proj=utm +zone=32\" projected_raster <- projectRaster(era5_precip, crs = sr) ## Error in methods::extends(class(x), \"CRS\"): object 'era5_precip' not found crop_era5 <- mask(projected_raster, norway_polygon) ## Error in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'mask': object 'projected_raster' not found crop_era5 <- terra::crop(crop_era5, extent(norway_polygon), snap=\"near\") ## Error in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'crop': object 'crop_era5' not found leaflet(sfc, width=750, height=500) %>%    addTiles() %>%   addRasterImage(x = crop_era5$X2021.10.01,                   opacity = 1) %>%   addPolygons(stroke = FALSE, smoothFactor = 0.3,               fillColor = \"red\",               fillOpacity = 0.5) ## Error in stopifnot(inherits(x, \"RasterLayer\")): object 'crop_era5' not found rsdf <- data.frame(rasterToPoints(crop_era5)) ## Error in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'nlayers': object 'crop_era5' not found rsdf <- melt(setDT(rsdf), id.vars=c(\"x\",\"y\"), variable.name = \"time\") ## Error: Cannot find symbol rsdf rsdf$time <- sapply(rsdf$time, function(t) {   as.integer(strsplit(as.character(t), '\\\\.')[[1]][3]) }) ## Error in lapply(X = X, FUN = FUN, ...): object 'rsdf' not found rsdf <- setNames(rsdf, c('x', 'y', 'time', 'precip')) ## Error in setNames(rsdf, c(\"x\", \"y\", \"time\", \"precip\")): object 'rsdf' not found # TODO : this needs to be moved to 4dm latlong_to_utm <- function(lat, lon) {      # Create a spatial points object with input coordinates   input_point <- SpatialPoints(matrix(c(lon, lat), ncol = 2), proj4string = CRS(\"+proj=longlat +datum=WGS84\"))      # Define the UTM zone number and hemisphere for the output coordinate system   utm_zone <- as.character((floor((lon + 180)/6) %% 60) + 1)   utm_hemisphere <- ifelse(lat < 0, \"S\", \"N\")   utm_proj_string <- paste0(\"+proj=utm +zone=\", utm_zone, \" +\", utm_hemisphere)      # Convert the input point to the output coordinate system   output_point <- spTransform(input_point, CRS(utm_proj_string))      # Return the UTM coordinates as a vector   return(c(output_point@coords[,1], output_point@coords[,2])) } pixel_coords <- unique(sp::coordinates(crop_era5)) ## Error in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'unique': error in evaluating the argument 'obj' in selecting a method for function 'coordinates': object 'crop_era5' not found s13_utm <- latlong_to_utm(lat=stream_gauge_13$latitude                           ,lon=stream_gauge_13$longitude) s14_utm <- latlong_to_utm(lat=stream_gauge_14$latitude                           ,lon=stream_gauge_14$longitude)  s13 <- data.frame(x=s13_utm[[1]], y=s13_utm[[2]]) s14 <- data.frame(x=s14_utm[[1]], y=s14_utm[[2]]) pixel_dist_gauge<-data.frame(x=pixel_coords[,1],                              y=pixel_coords[,2],                              dist_s1=rep(NA,nrow(pixel_coords)),                              dist_s2=rep(NA,nrow(pixel_coords)),                              min_s=rep(NA,nrow(pixel_coords)),                              streamflow=rep(NA, nrow(pixel_coords)) ) ## Error in data.frame(x = pixel_coords[, 1], y = pixel_coords[, 2], dist_s1 = rep(NA, : object 'pixel_coords' not found for (i in 1:nrow(pixel_coords)){   pixel_dist_gauge$dist_s1[i]<-sqrt((pixel_dist_gauge$x[i]-s13$x)^2+                                       (pixel_dist_gauge$y[i]-s13$y)^2)      pixel_dist_gauge$dist_s2[i]<-sqrt((pixel_dist_gauge$x[i]-s14$x)^2+                                       (pixel_dist_gauge$y[i]-s14$y)^2)   pixel_dist_gauge$min_s[i]<-which.min(c(pixel_dist_gauge$dist_s1[i],                                          pixel_dist_gauge$dist_s2[i])) } ## Error in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'nrow': object 'pixel_coords' not found # the dataframe pixel_dist_gauge then needs to be replicated by the number of time points  n.time <- nlayers(era5_precip) ## Error in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'nlayers': object 'era5_precip' not found pixel_dist_gauge<-do.call(rbind, replicate(n.time, pixel_dist_gauge, simplify=FALSE)) ## Error in integer(n): object 'n.time' not found pixel_dist_gauge$time<-rep(1:n.time,each=nrow(pixel_coords)) ## Error in eval(expr, envir, enclos): object 'n.time' not found streamdata <- list(data_13, data_14) ## Error in eval(expr, envir, enclos): object 'data_13' not found get_stream_data <- function(row) {   which_stream_data <- streamdata[[row['min_s']]]   data_at_row_time_index <- which_stream_data[which_stream_data[,c(\"time_index\")] == row['time'],]   return(data_at_row_time_index$value) } pixel_dist_gauge$streamflow <- apply(pixel_dist_gauge,1,get_stream_data) ## Error in apply(pixel_dist_gauge, 1, get_stream_data): object 'pixel_dist_gauge' not found inla_data<-merge(pixel_dist_gauge,rsdf,by=c(\"time\",\"x\",\"y\")) ## Error in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'merge': object 'pixel_dist_gauge' not found inla_data <- subset(inla_data, select=c('x', 'y', 'streamflow', 'precip', 'time')) ## Error in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'subset': object 'inla_data' not found sp::coordinates(inla_data)<- c(\"x\",\"y\") ## Error in sp::coordinates(inla_data) <- c(\"x\", \"y\"): object 'inla_data' not found head(inla_data@data) ## Error in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'head': object 'inla_data' not found head(inla_data@coords) ## Error in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'head': object 'inla_data' not found"},{"path":"/articles/hydro.html","id":"creating-the-bhm-using-4d-modeller","dir":"Articles","previous_headings":"","what":"Creating the BHM using 4D-Modeller","title":"Hydrology","text":"order model 4D-Modeller need : create spacial mesh SPDE model can evaluated build SPDE model Define process evolves time","code":""},{"path":"/articles/hydro.html","id":"mesh-resolution","dir":"Articles","previous_headings":"Creating the BHM using 4D-Modeller","what":"mesh resolution","title":"Hydrology","text":"Create triangulated mesh study region","code":"# TODO : it would be nice to map the mesh onto the dataset using the mapmaker function i am suggesting e <- crop_era5@extent ## Error in eval(expr, envir, enclos): object 'crop_era5' not found resolution <- res(crop_era5) ## Error in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'res': object 'crop_era5' not found xres <- resolution[1]*2 ## Error in resolution[1]: object of type 'closure' is not subsettable yres <- resolution[2]*2 ## Error in resolution[2]: object of type 'closure' is not subsettable xy <- sp::coordinates(crop_era5) ## Error in h(simpleError(msg, call)): error in evaluating the argument 'obj' in selecting a method for function 'coordinates': object 'crop_era5' not found # maybe this makes the mesh smaller lol xy <- xy[seq(1, nrow(xy), by = 2), ] ## Error in eval(expr, envir, enclos): object 'xy' not found # mesh <- INLA::inla.mesh.2d(loc=xy, max.edge=c(xres*1000, xres*10000), cutoff=2000, crs=sr) # mesh <- INLA::inla.mesh.2d(loc=xy, max.edge=c(xres*1, xres*100), cutoff=2000, crs=sr) mesh <- INLA::inla.mesh.2d(loc=xy, max.edge=c(xres*1, xres*100), cutoff=75, crs=sr) ## Error in xres * 1: non-numeric argument to binary operator plot(mesh) ## Error in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'plot': object 'mesh' not found"},{"path":"/articles/hydro.html","id":"priors","dir":"Articles","previous_headings":"Creating the BHM using 4D-Modeller","what":"priors","title":"Hydrology","text":"priors describing unobserved variannce distributed region. , far away center process cease spatially correlate surrounding environment. put different way, raining hill top, far hill must notice ’s raining . case, set prior range 20km. model time dependency. order fit model, also need define temporal index number discrete time points want model. INLA model requires time indicies must integer starting 1.","code":"# prior.range<-0.296 # the prior range is the distance that the process should stop effecting, so in this case it is currently 20km away from the node center prior.range<-20.0 spde <- inla.spde2.pcmatern(   mesh = mesh,    prior.range = c(prior.range, 0.5),    prior.sigma = c(1, 0.01)  ) ## Error in inla.spde2.pcmatern(mesh = mesh, prior.range = c(prior.range, : object 'mesh' not found rhoprior <- base::list(theta = list(prior = 'pccor1', param = c(0, 0.9))) group.index <- inla_data@data$time ## Error in eval(expr, envir, enclos): object 'inla_data' not found n_groups<-length(unique(group.index)) ## Error in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'unique': object 'group.index' not found"},{"path":"/articles/hydro.html","id":"model-inference","dir":"Articles","previous_headings":"Creating the BHM using 4D-Modeller","what":"Model Inference","title":"Hydrology","text":"model predicts streamflow given precipitation fixed effect SPDE model assumes correlation structure streamflow data due unobserved variables. Likely unobserved co-variates local elevation, temperature, soil permeability, presence vegetation. now time see output","code":"formula1 <- streamflow ~ 0 + Intercept + precip  formula2 <- streamflow ~ 0 + Intercept + precip +   f(main=coordinates,     model=spde,     group=group.index,     ngroup=n_groups,     control.group=list(model=\"ar1\",                        hyper=rhoprior)) inlabru.model1 <- bru(formula1, data = inla_data,                    family = \"gaussian\",                    options = list(                      verbose = TRUE                    ) ) ## Error in bru(formula1, data = inla_data, family = \"gaussian\", options = list(verbose = TRUE)): object 'inla_data' not found inlabru.model2 <- bru(formula2, data = inla_data,                    family = \"gaussian\",                    options = list(                      verbose = TRUE                    ) ) ## Error in bru(formula2, data = inla_data, family = \"gaussian\", options = list(verbose = TRUE)): object 'inla_data' not found # TODO : this could be where the model_eval function i recommend could be used summary(inlabru.model1) ## Error in h(simpleError(msg, call)): error in evaluating the argument 'object' in selecting a method for function 'summary': object 'inlabru.model1' not found summary(inlabru.model2) ## Error in h(simpleError(msg, call)): error in evaluating the argument 'object' in selecting a method for function 'summary': object 'inlabru.model2' not found"},{"path":"/articles/processing.html","id":"installation","dir":"Articles","previous_headings":"","what":"Installation","title":"Quickstart","text":"get 4D Modeller processing installed first need make sure recent version R installed. easiest way install RStudio. Next clone repository move directory. Next need install INLA, won’t installed default install processing package. open R console RStudio console run Next can install processing package local directory. install processing package dependencies. ’re now ready move onto one tutorials.","code":"git clone https://github.com/GlobalMass/processing.git cd processing install.packages(\"INLA\",repos=c(getOption(\"repos\"),INLA=\"https://inla.r-inla-download.org/R/stable\"), dep=TRUE) install.packages(\".\", repos = NULL, type = \"source\")"},{"path":"/articles/quickstart.html","id":"installation","dir":"Articles","previous_headings":"","what":"Installation","title":"Quickstart","text":"get 4D Modeller processing installed first need make sure recent version R installed. easiest way install RStudio. Next clone repository move directory. Next need install INLA, won’t installed default install processing package. open R console RStudio console run Next can install processing package local directory. install processing package dependencies. ’re now ready move onto GlobalMass tutorial.","code":"git clone https://github.com/GlobalMass/processing.git cd processing install.packages(\"INLA\",repos=c(getOption(\"repos\"),INLA=\"https://inla.r-inla-download.org/R/stable\"), dep=TRUE) install.packages(\".\", repos = NULL, type = \"source\")"},{"path":"/articles/run_bhm_gic.html","id":"input-files","dir":"Articles","previous_headings":"","what":"Input files","title":"Running BHM for GlobalMass","text":"first need input files created previous tutorial. Hopefully intermediate_data folder looks like doesn’t, please go back make sure ’ve run step tutorial.","code":"ls intermediate_data"},{"path":"/articles/run_bhm_gic.html","id":"files-and-arguments","dir":"Articles","previous_headings":"","what":"Files and arguments","title":"Running BHM for GlobalMass","text":"tutorial ’ll run BHM glacier icesheet (GIC) data. means need paths data files intermediate_data/GIC folder pass run_bhm_gic function. computer data folder home directory 4D_modeller. Let’s start creating paths files. also need select GRACE data centre want use, choice GSFC JPL. can see filename data_filepath ’re using GSFC data , set GRACE_dc like . also want set output_folder save model run summary model outputs . ’ll create folder called model_output within folder called GIC. -p flag mkdir means make parent directory GIC directory .","code":"?run_bhm_gic mesh_filepath <- \"mesh_GIC.Rds\" data_filepath <- \"EWH_GSFC_grace_global_GMGIA_to2015.RData\" grace_data_filepath <- \"grace_data.RData\" GRACE_dc <- \"GSFC\" mkdir -p model_output/GIC"},{"path":"/articles/run_bhm_gic.html","id":"running-the-bhm","dir":"Articles","previous_headings":"","what":"Running the BHM","title":"Running BHM for GlobalMass","text":"’re now ready run BHM. see model run information printed screen. includes … function completes returns list output filepaths. makes easy view output model.","code":"output_filepaths <- run_bhm_gic(mesh_filepath = mesh_filepath,                                 data_filepath = data_filepath,                                 grace_data_filepath = grace_data_filepath,                                 GRACE_dc = GRACE_dc,                                 output_folder = \"model_output/GIC\")"},{"path":"/articles/run_bhm_gic.html","id":"examine-model-output","dir":"Articles","previous_headings":"Running the BHM","what":"Examine model output","title":"Running BHM for GlobalMass","text":"Returned run_bhm_gic function get three filepaths first glacier data, second glacier data CSV format third model output rds file. Let’s open file take look. First take third filepath list, load data using readRDS. can see …","code":"output_filepaths run_data_filepath <- output_filepaths[3] model_run_data <- readRDS(run_data_filepath) model_run_data"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Gareth Jones. Author, maintainer. Sam Royston. Author. Xueqing Yin. Author. John Aiken. Author. Jonathan Bamber. Author.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Jones G, Royston S, Yin X, Aiken J, Bamber J (2023). fdmr: 4D Modeller project. R package version 0.0.1, https://github.com/GlobalMass/processing.","code":"@Manual{,   title = {fdmr: 4D Modeller project},   author = {Gareth Jones and Sam Royston and Xueqing Yin and John Aiken and Jonathan Bamber},   year = {2023},   note = {R package version 0.0.1},   url = {https://github.com/GlobalMass/processing}, }"},{"path":"/index.html","id":"id_4d-modeller","dir":"","previous_headings":"","what":"4D Modeller project","title":"4D Modeller project","text":"repository contains code 4DModeller project.","code":""},{"path":"/index.html","id":"quickstart","dir":"","previous_headings":"","what":"Quickstart","title":"4D Modeller project","text":"R package currently called fdmr ’s easy write ’s currently package name find. get 4D Modeller installed first need make sure recent version R installed. easiest way install RStudio. Next clone repository move directory Next, start R session use renv install packages required fdmr. Next need install INLA, won’t installed default install fdmr package. Next can install fdmr package local directory source package. install fdmr package dependencies.","code":"# git clone https://github.com/4DModeller/fdmr.git # cd fdmr # renv::restore() # install.packages(\"INLA\", repos=c(getOption(\"repos\"), INLA=\"https://inla.r-inla-download.org/R/stable\"), dep=TRUE) # install.packages(\".\", repos = NULL, type = \"source\")"},{"path":"/reference/clean_path.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns a ~ expanded absolute path — clean_path","title":"Returns a ~ expanded absolute path — clean_path","text":"Returns ~ expanded absolute path","code":""},{"path":"/reference/clean_path.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns a ~ expanded absolute path — clean_path","text":"","code":"clean_path(path, check_exists = FALSE)"},{"path":"/reference/clean_path.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Returns a ~ expanded absolute path — clean_path","text":"path Path clean check_exists Check path exists, error ","code":""},{"path":"/reference/clean_path.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Returns a ~ expanded absolute path — clean_path","text":"fs::path Expanded absolute path","code":""},{"path":"/reference/create_config.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a user configuration file — create_config","title":"Create a user configuration file — create_config","text":"Create user configuration file","code":""},{"path":"/reference/create_config.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a user configuration file — create_config","text":"","code":"create_config(tutorial_data = \"fdmr_example_data\", silent = FALSE)"},{"path":"/reference/create_config.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a user configuration file — create_config","text":"tutorial_data Path tutorial data folder silent Create configuration file silently","code":""},{"path":"/reference/create_fibo_sphere.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Fibonacci points on the sphere\nReference (note that points generated from 2D are slightly different from 3D)\nMeasurement of Areas on a Sphere Using Fibonacci and Latitude–Longitude Lattices (2010) — create_fibo_sphere","title":"Generate Fibonacci points on the sphere\nReference (note that points generated from 2D are slightly different from 3D)\nMeasurement of Areas on a Sphere Using Fibonacci and Latitude–Longitude Lattices (2010) — create_fibo_sphere","text":"Generate Fibonacci points sphere Reference (note points generated 2D slightly different 3D) Measurement Areas Sphere Using Fibonacci Latitude–Longitude Lattices (2010)","code":""},{"path":"/reference/create_fibo_sphere.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Fibonacci points on the sphere\nReference (note that points generated from 2D are slightly different from 3D)\nMeasurement of Areas on a Sphere Using Fibonacci and Latitude–Longitude Lattices (2010) — create_fibo_sphere","text":"","code":"create_fibo_sphere(N = 1000L, LL = TRUE, L0 = FALSE)"},{"path":"/reference/create_fibo_sphere.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Fibonacci points on the sphere\nReference (note that points generated from 2D are slightly different from 3D)\nMeasurement of Areas on a Sphere Using Fibonacci and Latitude–Longitude Lattices (2010) — create_fibo_sphere","text":"N number points create LL TRUE lon/lat coords L0 TRUE lon 0 360","code":""},{"path":"/reference/create_fibo_sphere.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Fibonacci points on the sphere\nReference (note that points generated from 2D are slightly different from 3D)\nMeasurement of Areas on a Sphere Using Fibonacci and Latitude–Longitude Lattices (2010) — create_fibo_sphere","text":"matrix points","code":""},{"path":"/reference/create_gic_mesh.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a GIC mesh and output the mesh and mesh triangles to files\ncreated in the output_folder — create_gic_mesh","title":"Create a GIC mesh and output the mesh and mesh triangles to files\ncreated in the output_folder — create_gic_mesh","text":"Create GIC mesh output mesh mesh triangles files created output_folder","code":""},{"path":"/reference/create_gic_mesh.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a GIC mesh and output the mesh and mesh triangles to files\ncreated in the output_folder — create_gic_mesh","text":"","code":"create_gic_mesh(shapefile_path, output_folder)"},{"path":"/reference/create_gic_mesh.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a GIC mesh and output the mesh and mesh triangles to files\ncreated in the output_folder — create_gic_mesh","text":"shapefile_path Path shapefile output_folder Output folder","code":""},{"path":"/reference/create_gic_mesh.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a GIC mesh and output the mesh and mesh triangles to files\ncreated in the output_folder — create_gic_mesh","text":"vector: Mesh filepath mesh triangles filepath","code":""},{"path":"/reference/create_icesheet_mesh.html","id":null,"dir":"Reference","previous_headings":"","what":"Create mesh for icesheets and write the mesh and mesh triangles to\nthe output folder — create_icesheet_mesh","title":"Create mesh for icesheets and write the mesh and mesh triangles to\nthe output folder — create_icesheet_mesh","text":"Create mesh icesheets write mesh mesh triangles output folder","code":""},{"path":"/reference/create_icesheet_mesh.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create mesh for icesheets and write the mesh and mesh triangles to\nthe output folder — create_icesheet_mesh","text":"","code":"create_icesheet_mesh(   land_shapefile_path,   gic_shapefile_path,   icesheet_shapefile_path,   output_folder )"},{"path":"/reference/create_icesheet_mesh.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create mesh for icesheets and write the mesh and mesh triangles to\nthe output folder — create_icesheet_mesh","text":"land_shapefile_path Path land shapefile gic_shapefile_path Path GIC shapefile icesheet_shapefile_path Path icesheet shapefile output_folder Path output folder","code":""},{"path":"/reference/create_icesheet_mesh.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create mesh for icesheets and write the mesh and mesh triangles to\nthe output folder — create_icesheet_mesh","text":"vector: Mesh filepath mesh triangles filepath","code":""},{"path":"/reference/create_land_mesh.html","id":null,"dir":"Reference","previous_headings":"","what":"Create mesh for land and write the mesh and mesh triangles to\nthe output folder — create_land_mesh","title":"Create mesh for land and write the mesh and mesh triangles to\nthe output folder — create_land_mesh","text":"Create mesh land write mesh mesh triangles output folder","code":""},{"path":"/reference/create_land_mesh.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create mesh for land and write the mesh and mesh triangles to\nthe output folder — create_land_mesh","text":"","code":"create_land_mesh(   land_shapefile_path,   gic_shapefile_path,   icesheet_shapefile_path,   output_folder )"},{"path":"/reference/create_land_mesh.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create mesh for land and write the mesh and mesh triangles to\nthe output folder — create_land_mesh","text":"land_shapefile_path Path land shapefile gic_shapefile_path Path GIC shapefile icesheet_shapefile_path Path icesheet shapefile output_folder Output folder mesh files","code":""},{"path":"/reference/create_land_mesh.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create mesh for land and write the mesh and mesh triangles to\nthe output folder — create_land_mesh","text":"vector: Mesh filepath mesh triangles filepath","code":""},{"path":"/reference/create_mesh.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a mesh for ... — create_mesh","title":"Create a mesh for ... — create_mesh","text":"Create mesh ...","code":""},{"path":"/reference/create_mesh.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a mesh for ... — create_mesh","text":"","code":"create_mesh(input_filepath, output_folder)"},{"path":"/reference/create_mesh.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a mesh for ... — create_mesh","text":"input_filepath Path input file output_folder Output folder path","code":""},{"path":"/reference/create_ocean_mesh.html","id":null,"dir":"Reference","previous_headings":"","what":"Create mesh for ocean and write the mesh and mesh triangles to\nthe output folder — create_ocean_mesh","title":"Create mesh for ocean and write the mesh and mesh triangles to\nthe output folder — create_ocean_mesh","text":"Create mesh ocean write mesh mesh triangles output folder","code":""},{"path":"/reference/create_ocean_mesh.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create mesh for ocean and write the mesh and mesh triangles to\nthe output folder — create_ocean_mesh","text":"","code":"create_ocean_mesh(shapefile_folder, output_folder)"},{"path":"/reference/create_ocean_mesh.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create mesh for ocean and write the mesh and mesh triangles to\nthe output folder — create_ocean_mesh","text":"shapefile_folder Folder containing shapefiles output_folder Output folder mesh files","code":""},{"path":"/reference/create_ocean_mesh.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create mesh for ocean and write the mesh and mesh triangles to\nthe output folder — create_ocean_mesh","text":"list: Mesh filepath mesh triangles filepath","code":""},{"path":"/reference/dt_Omega.html","id":null,"dir":"Reference","previous_headings":"","what":"Creates a legal Omega from just a list of subdomains — dt_Omega","title":"Creates a legal Omega from just a list of subdomains — dt_Omega","text":"fun creates legal Omega just list subdomains make sure every triangle included somewhere make sure multiplicities","code":""},{"path":"/reference/dt_Omega.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Creates a legal Omega from just a list of subdomains — dt_Omega","text":"","code":"dt_Omega(list_of_subdomains, mesh)"},{"path":"/reference/dt_Omega.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Creates a legal Omega from just a list of subdomains — dt_Omega","text":"list_of_subdomains must list numeric 'c(...)' mesh Mesh object","code":""},{"path":"/reference/dt_Omega.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Creates a legal Omega from just a list of subdomains — dt_Omega","text":"list","code":""},{"path":"/reference/dt_mesh_addon_posTri.html","id":null,"dir":"Reference","previous_headings":"","what":"Add new attributes to the mesh object — dt_mesh_addon_posTri","title":"Add new attributes to the mesh object — dt_mesh_addon_posTri","text":"Add new attributes mesh object","code":""},{"path":"/reference/dt_mesh_addon_posTri.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add new attributes to the mesh object — dt_mesh_addon_posTri","text":"","code":"dt_mesh_addon_posTri(mesh, globe = FALSE)"},{"path":"/reference/dt_mesh_addon_posTri.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add new attributes to the mesh object — dt_mesh_addon_posTri","text":"mesh 2D INLA mesh globe TRUE full globe","code":""},{"path":"/reference/dt_mesh_addon_posTri.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add new attributes to the mesh object — dt_mesh_addon_posTri","text":"Mesh new attributes added","code":""},{"path":"/reference/fit_sp.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit spatio-temporal data — fit_sp","title":"Fit spatio-temporal data — fit_sp","text":"Fit spatio-temporal data","code":""},{"path":"/reference/fit_sp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit spatio-temporal data — fit_sp","text":"","code":"fit_sp(   formula,   data,   family,   E,   pred.link,   reordering,   int.strategy,   mode,   control.family = list(link = \" \"),   verbose = FALSE )"},{"path":"/reference/fit_sp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit spatio-temporal data — fit_sp","text":"verbose","code":""},{"path":"/reference/get_tutorial_data_folder.html","id":null,"dir":"Reference","previous_headings":"","what":"Get canonical tutorial data path — get_tutorial_data_folder","title":"Get canonical tutorial data path — get_tutorial_data_folder","text":"Get canonical tutorial data path","code":""},{"path":"/reference/get_tutorial_data_folder.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get canonical tutorial data path — get_tutorial_data_folder","text":"","code":"get_tutorial_data_folder()"},{"path":"/reference/get_tutorial_data_folder.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get canonical tutorial data path — get_tutorial_data_folder","text":"fs::path: Path tutorial data folder","code":""},{"path":"/reference/get_tutorial_datapath.html","id":null,"dir":"Reference","previous_headings":"","what":"Return the filepath for a tutorial data file. — get_tutorial_datapath","title":"Return the filepath for a tutorial data file. — get_tutorial_datapath","text":"Return filepath tutorial data file.","code":""},{"path":"/reference/get_tutorial_datapath.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return the filepath for a tutorial data file. — get_tutorial_datapath","text":"","code":"get_tutorial_datapath(filename)"},{"path":"/reference/get_tutorial_datapath.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return the filepath for a tutorial data file. — get_tutorial_datapath","text":"filename Name file","code":""},{"path":"/reference/get_tutorial_datapath.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return the filepath for a tutorial data file. — get_tutorial_datapath","text":"fs::path Full filepath","code":""},{"path":"/reference/get_tutorial_folderpath.html","id":null,"dir":"Reference","previous_headings":"","what":"Get path to folder containing multiple files we want to use.\nThis is used for opening shapefiles in the meshing functions. — get_tutorial_folderpath","title":"Get path to folder containing multiple files we want to use.\nThis is used for opening shapefiles in the meshing functions. — get_tutorial_folderpath","text":"Get path folder containing multiple files want use. used opening shapefiles meshing functions.","code":""},{"path":"/reference/get_tutorial_folderpath.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get path to folder containing multiple files we want to use.\nThis is used for opening shapefiles in the meshing functions. — get_tutorial_folderpath","text":"","code":"get_tutorial_folderpath(data_type)"},{"path":"/reference/get_tutorial_folderpath.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get path to folder containing multiple files we want to use.\nThis is used for opening shapefiles in the meshing functions. — get_tutorial_folderpath","text":"data_type Data type","code":""},{"path":"/reference/get_tutorial_folderpath.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get path to folder containing multiple files we want to use.\nThis is used for opening shapefiles in the meshing functions. — get_tutorial_folderpath","text":"fs::path: Path data folder","code":""},{"path":"/reference/load_tutorial_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Load data from the tutorial data store — load_tutorial_data","title":"Load data from the tutorial data store — load_tutorial_data","text":"Load data tutorial data store","code":""},{"path":"/reference/load_tutorial_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load data from the tutorial data store — load_tutorial_data","text":"","code":"load_tutorial_data(filename)"},{"path":"/reference/load_tutorial_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load data from the tutorial data store — load_tutorial_data","text":"filename Name file","code":""},{"path":"/reference/load_tutorial_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Load data from the tutorial data store — load_tutorial_data","text":"loaded object","code":""},{"path":"/reference/mesh_subset.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the subset of the mesh for buiding the Q — mesh_subset","title":"Get the subset of the mesh for buiding the Q — mesh_subset","text":"Get subset mesh buiding Q","code":""},{"path":"/reference/mesh_subset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the subset of the mesh for buiding the Q — mesh_subset","text":"","code":"mesh_subset(mesh, Omega, i = 2)"},{"path":"/reference/mesh_subset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the subset of the mesh for buiding the Q — mesh_subset","text":"mesh Mesh Omega Omega triangle Id subset","code":""},{"path":"/reference/mesh_subset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the subset of the mesh for buiding the Q — mesh_subset","text":"Mesh subset","code":""},{"path":"/reference/piecewise_interpolation.html","id":null,"dir":"Reference","previous_headings":"","what":"Piecewise interpolation of a function using tent functions. — piecewise_interpolation","title":"Piecewise interpolation of a function using tent functions. — piecewise_interpolation","text":"Piecewise interpolation function using tent functions.","code":""},{"path":"/reference/piecewise_interpolation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Piecewise interpolation of a function using tent functions. — piecewise_interpolation","text":"","code":"piecewise_interpolation(X, ds = 1, first = NULL, knots = NULL)"},{"path":"/reference/piecewise_interpolation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Piecewise interpolation of a function using tent functions. — piecewise_interpolation","text":"X data-frame fields 'x' 'y' ds denotes spacing knots (1 default) first denotes location first knot (e.g. 2006) knots pre-placed knots","code":""},{"path":"/reference/piecewise_interpolation.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Piecewise interpolation of a function using tent functions. — piecewise_interpolation","text":"list maximum likelihood estimate weights trend estimates","code":""},{"path":"/reference/plot_barchart.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot a bar chart using ggplot2 — plot_barchart","title":"Plot a bar chart using ggplot2 — plot_barchart","text":"Plot bar chart using ggplot2","code":""},{"path":"/reference/plot_barchart.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot a bar chart using ggplot2 — plot_barchart","text":"","code":"plot_barchart(   data,   x,   y,   breaks,   x_label,   y_label,   fill = \"pink\",   colour = \"blue\" )"},{"path":"/reference/plot_barchart.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot a bar chart using ggplot2 — plot_barchart","text":"data Data plot x x-axis data y y-axis data breaks Break points x_label x-axis label y_label y-axis label fill Fill colour colour Line colour","code":""},{"path":"/reference/plot_barchart.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot a bar chart using ggplot2 — plot_barchart","text":"ggplot","code":""},{"path":"/reference/plot_boxplot.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot a boxplot using ggplot2 — plot_boxplot","title":"Plot a boxplot using ggplot2 — plot_boxplot","text":"Plot boxplot using ggplot2","code":""},{"path":"/reference/plot_boxplot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot a boxplot using ggplot2 — plot_boxplot","text":"","code":"plot_boxplot(data, x, y, breaks, x_label, y_label)"},{"path":"/reference/plot_boxplot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot a boxplot using ggplot2 — plot_boxplot","text":"data Data plot x x-axis data y y-axis data breaks Break points x_label x-axis label y_label y-axis label","code":""},{"path":"/reference/plot_boxplot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot a boxplot using ggplot2 — plot_boxplot","text":"ggplot","code":""},{"path":"/reference/plot_line_average.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a line plot with a confidence interval. — plot_line_average","title":"Create a line plot with a confidence interval. — plot_line_average","text":"Create line plot confidence interval.","code":""},{"path":"/reference/plot_line_average.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a line plot with a confidence interval. — plot_line_average","text":"","code":"plot_line_average(   data,   x,   y1,   y2,   y3,   breaks,   x_label,   y_label,   y1_colour = \"blue\",   y2_colour = \"red\",   y3_colour = \"red\",   x_lim = NULL,   y_lim = NULL )"},{"path":"/reference/plot_line_average.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a line plot with a confidence interval. — plot_line_average","text":"data Data plot x x-axis data y1 y1 line data - solid y2 y2 line data - dashed y3 y3 line data - dashed breaks Breaks vector x_label x-axis label y_label y-axis label y1_colour Colour y1 y2_colour Colour y2 y3_colour Colour y3 x_lim x-axis limits vector e.g. c(0, 0.1) y_lim y-axis limits vector e.g. c(0, 1.0)","code":""},{"path":"/reference/plot_line_average.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a line plot with a confidence interval. — plot_line_average","text":"ggplot","code":""},{"path":"/reference/plot_map.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a simple Leaflet map from data — plot_map","title":"Create a simple Leaflet map from data — plot_map","text":"Create simple Leaflet map data","code":""},{"path":"/reference/plot_map.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a simple Leaflet map from data — plot_map","text":"","code":"plot_map(   data,   domain,   palette = \"YlOrRd\",   colour = \"grey\",   legend_values = NULL,   legend_title = NULL,   add_scale_bar = FALSE,   polygon_fill_opacity = 0.75,   fill_colour_weight = 1 )"},{"path":"/reference/plot_map.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a simple Leaflet map from data — plot_map","text":"data Data plot domain Domain map palette Palette, example YlOrRd Reds legend_values Values legend legend_title Title legend add_scale_bar Add scale bar TRUE polygon_fill_opacity Leaflet polygon fill opacity, float 0 1.0, passed fillOpacity leaflet::addPolygons fill_colour_weight Polygon colour weight, float 0 1.0, Passed weight argument addPolygons","code":""},{"path":"/reference/plot_mesh.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot a mesh — plot_mesh","title":"Plot a mesh — plot_mesh","text":"Plot mesh","code":""},{"path":"/reference/plot_mesh.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot a mesh — plot_mesh","text":"","code":"plot_mesh(mesh, point_data, point_colour = \"blue\", cex = 0.1)"},{"path":"/reference/plot_mesh.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot a mesh — plot_mesh","text":"mesh Mesh data point_data Points data point_colour Colour points cex Point size magnifier","code":""},{"path":"/reference/plot_timeseries.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot timeseries data — plot_timeseries","title":"Plot timeseries data — plot_timeseries","text":"Plot timeseries data","code":""},{"path":"/reference/plot_timeseries.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot timeseries data — plot_timeseries","text":"","code":"plot_timeseries(   data,   x,   y,   breaks,   x_label = NULL,   y_label = NULL,   title = NULL,   line_colour = \"blue\",   horizontal_y = NULL,   vertical_x = NULL,   x_lim = NULL,   y_lim = NULL )"},{"path":"/reference/plot_timeseries.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot timeseries data — plot_timeseries","text":"data Data plot x x-axis data y y-axis data breaks Date break points x_label x-axis label y_label y-axis label title Figure title line_colour Line colour horizontal_y y-intercept horizontal line vertical_x x-intercept vertical line x_lim Limits x-axis continous scale, vector passed scale_x_continuous y_lim Limits y-axis continuous scale, vector passed scale_y_continous","code":""},{"path":"/reference/plot_timeseries.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot timeseries data — plot_timeseries","text":"ggplot","code":""},{"path":"/reference/poly_block.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate the integration grid for each polygon — poly_block","title":"Generate the integration grid for each polygon — poly_block","text":"Generate integration grid polygon","code":""},{"path":"/reference/poly_block.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate the integration grid for each polygon — poly_block","text":"","code":"poly_block(i, df_sp, dis = 1e+05)"},{"path":"/reference/poly_block.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate the integration grid for each polygon — poly_block","text":"Polygon number df_sp Spatial Polygon DataFrame dis Dis","code":""},{"path":"/reference/poly_block.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate the integration grid for each polygon — poly_block","text":"list","code":""},{"path":"/reference/process_altimetry_proc_gmgia.html","id":null,"dir":"Reference","previous_headings":"","what":"Process GMGIA proc altimetry data. Outputs processed\ndata to the output_folder along with plots if plot_figures is TRUE. — process_altimetry_proc_gmgia","title":"Process GMGIA proc altimetry data. Outputs processed\ndata to the output_folder along with plots if plot_figures is TRUE. — process_altimetry_proc_gmgia","text":"Process GMGIA proc altimetry data. Outputs processed data output_folder along plots plot_figures TRUE.","code":""},{"path":"/reference/process_altimetry_proc_gmgia.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process GMGIA proc altimetry data. Outputs processed\ndata to the output_folder along with plots if plot_figures is TRUE. — process_altimetry_proc_gmgia","text":"","code":"process_altimetry_proc_gmgia(   input_filepath,   output_folder,   plot_figures = TRUE,   start_year = 2005,   end_year = 2016 )"},{"path":"/reference/process_altimetry_proc_gmgia.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process GMGIA proc altimetry data. Outputs processed\ndata to the output_folder along with plots if plot_figures is TRUE. — process_altimetry_proc_gmgia","text":"input_filepath Input filepath output_folder Output folder plot_figures TRUE plot figures output_folder start_year First year process end_year Last year process","code":""},{"path":"/reference/process_altimetry_proc_gmgia.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process GMGIA proc altimetry data. Outputs processed\ndata to the output_folder along with plots if plot_figures is TRUE. — process_altimetry_proc_gmgia","text":"str: Output filepath","code":""},{"path":"/reference/process_antarctic_altimetry.html","id":null,"dir":"Reference","previous_headings":"","what":"Process antarctic altimetry data for years between start_year and end_year.\nOutput file and figures (if plot_figures = TRUE) are created in output_folder. — process_antarctic_altimetry","title":"Process antarctic altimetry data for years between start_year and end_year.\nOutput file and figures (if plot_figures = TRUE) are created in output_folder. — process_antarctic_altimetry","text":"Process antarctic altimetry data years start_year end_year. Output file figures (plot_figures = TRUE) created output_folder.","code":""},{"path":"/reference/process_antarctic_altimetry.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process antarctic altimetry data for years between start_year and end_year.\nOutput file and figures (if plot_figures = TRUE) are created in output_folder. — process_antarctic_altimetry","text":"","code":"process_antarctic_altimetry(   input_filepath,   output_folder,   plot_figures = TRUE,   start_year = 2005,   end_year = 2016 )"},{"path":"/reference/process_antarctic_altimetry.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process antarctic altimetry data for years between start_year and end_year.\nOutput file and figures (if plot_figures = TRUE) are created in output_folder. — process_antarctic_altimetry","text":"input_filepath Input filepath output_folder Output folder plot_figures Plot figures, default = TRUE start_year First year process end_year Last year process","code":""},{"path":"/reference/process_antarctic_altimetry.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process antarctic altimetry data for years between start_year and end_year.\nOutput file and figures (if plot_figures = TRUE) are created in output_folder. — process_antarctic_altimetry","text":"str: Output filepath","code":""},{"path":"/reference/process_argo_flagged.html","id":null,"dir":"Reference","previous_headings":"","what":"Process flagged ARGO data. Outputs processed\ndata to the output_folder along with plots if plot_figures is TRUE. — process_argo_flagged","title":"Process flagged ARGO data. Outputs processed\ndata to the output_folder along with plots if plot_figures is TRUE. — process_argo_flagged","text":"Process flagged ARGO data. Outputs processed data output_folder along plots plot_figures TRUE.","code":""},{"path":"/reference/process_argo_flagged.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process flagged ARGO data. Outputs processed\ndata to the output_folder along with plots if plot_figures is TRUE. — process_argo_flagged","text":"","code":"process_argo_flagged(   input_filepath,   output_folder,   plot_figures = TRUE,   start_year = 2005,   end_year = 2016 )"},{"path":"/reference/process_argo_flagged.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process flagged ARGO data. Outputs processed\ndata to the output_folder along with plots if plot_figures is TRUE. — process_argo_flagged","text":"input_filepath Input filepath output_folder Output folder plot_figures TRUE plot figures output_folder start_year First year process end_year Last year process","code":""},{"path":"/reference/process_argo_flagged.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process flagged ARGO data. Outputs processed\ndata to the output_folder along with plots if plot_figures is TRUE. — process_argo_flagged","text":"str: Output filepath","code":""},{"path":"/reference/process_ewh_gsfc_grace.html","id":null,"dir":"Reference","previous_headings":"","what":"Process the EWH GSFC Grace global mascon data. Outputs processed\ndata to the output_folder along with plots if plot_figures is TRUE. — process_ewh_gsfc_grace","title":"Process the EWH GSFC Grace global mascon data. Outputs processed\ndata to the output_folder along with plots if plot_figures is TRUE. — process_ewh_gsfc_grace","text":"Process EWH GSFC Grace global mascon data. Outputs processed data output_folder along plots plot_figures TRUE.","code":""},{"path":"/reference/process_ewh_gsfc_grace.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process the EWH GSFC Grace global mascon data. Outputs processed\ndata to the output_folder along with plots if plot_figures is TRUE. — process_ewh_gsfc_grace","text":"","code":"process_ewh_gsfc_grace(   input_filepath,   output_folder,   plot_figures = TRUE,   start_year = 2005,   end_year = 2016 )"},{"path":"/reference/process_ewh_gsfc_grace.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process the EWH GSFC Grace global mascon data. Outputs processed\ndata to the output_folder along with plots if plot_figures is TRUE. — process_ewh_gsfc_grace","text":"input_filepath Input filepath output_folder Output folder plot_figures TRUE plot figures output_folder start_year First year process end_year Last year process","code":""},{"path":"/reference/process_ewh_gsfc_grace.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process the EWH GSFC Grace global mascon data. Outputs processed\ndata to the output_folder along with plots if plot_figures is TRUE. — process_ewh_gsfc_grace","text":"str: Output filepath","code":""},{"path":"/reference/process_ewh_jpl_grace.html","id":null,"dir":"Reference","previous_headings":"","what":"Process the EWH JPL Grace global mascon data. Outputs processed\ndata to the output_folder along with plots if plot_figures is TRUE. — process_ewh_jpl_grace","title":"Process the EWH JPL Grace global mascon data. Outputs processed\ndata to the output_folder along with plots if plot_figures is TRUE. — process_ewh_jpl_grace","text":"Process EWH JPL Grace global mascon data. Outputs processed data output_folder along plots plot_figures TRUE.","code":""},{"path":"/reference/process_ewh_jpl_grace.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process the EWH JPL Grace global mascon data. Outputs processed\ndata to the output_folder along with plots if plot_figures is TRUE. — process_ewh_jpl_grace","text":"","code":"process_ewh_jpl_grace(   input_filepath,   output_folder,   plot_figures = TRUE,   start_year = 2005,   end_year = 2016 )"},{"path":"/reference/process_ewh_jpl_grace.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process the EWH JPL Grace global mascon data. Outputs processed\ndata to the output_folder along with plots if plot_figures is TRUE. — process_ewh_jpl_grace","text":"input_filepath Input filepath output_folder Output folder plot_figures TRUE plot figures output_folder start_year First year process end_year Last year process","code":""},{"path":"/reference/process_ewh_jpl_grace.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process the EWH JPL Grace global mascon data. Outputs processed\ndata to the output_folder along with plots if plot_figures is TRUE. — process_ewh_jpl_grace","text":"str: Output filepath","code":""},{"path":"/reference/process_greenland_altimetry.html","id":null,"dir":"Reference","previous_headings":"","what":"Process Greenland altimetry data and output the data to\noutput_folder. If plot_figures is TRUE figures are created\nin the same folder. — process_greenland_altimetry","title":"Process Greenland altimetry data and output the data to\noutput_folder. If plot_figures is TRUE figures are created\nin the same folder. — process_greenland_altimetry","text":"Process Greenland altimetry data output data output_folder. plot_figures TRUE figures created folder.","code":""},{"path":"/reference/process_greenland_altimetry.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process Greenland altimetry data and output the data to\noutput_folder. If plot_figures is TRUE figures are created\nin the same folder. — process_greenland_altimetry","text":"","code":"process_greenland_altimetry(   input_filepath,   output_folder,   plot_figures = TRUE,   start_year = 2005,   end_year = 2015 )"},{"path":"/reference/process_greenland_altimetry.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process Greenland altimetry data and output the data to\noutput_folder. If plot_figures is TRUE figures are created\nin the same folder. — process_greenland_altimetry","text":"input_filepath Input filepath output_folder Output folder plot_figures Plot figures TRUE start_year First year process end_year Last year process","code":""},{"path":"/reference/process_greenland_altimetry.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process Greenland altimetry data and output the data to\noutput_folder. If plot_figures is TRUE figures are created\nin the same folder. — process_greenland_altimetry","text":"str: Output filepath","code":""},{"path":"/reference/process_hugonnet_tiles.html","id":null,"dir":"Reference","previous_headings":"","what":"Processing Hugonnet tiles data — process_hugonnet_tiles","title":"Processing Hugonnet tiles data — process_hugonnet_tiles","text":"Processing Hugonnet tiles data","code":""},{"path":"/reference/process_hugonnet_tiles.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Processing Hugonnet tiles data — process_hugonnet_tiles","text":"","code":"process_hugonnet_tiles(input_filepath, output_folder, plot_figures = FALSE)"},{"path":"/reference/process_hugonnet_tiles.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Processing Hugonnet tiles data — process_hugonnet_tiles","text":"input_filepath Input data filepath output_folder Folder output data plot_figures TRUE plot figures","code":""},{"path":"/reference/process_hugonnet_tiles.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Processing Hugonnet tiles data — process_hugonnet_tiles","text":"Output filepaths","code":""},{"path":"/reference/process_ocean_argo.html","id":null,"dir":"Reference","previous_headings":"","what":"Process flagged ARGO data. Outputs processed\ndata to the output_folder along with plots if plot_figures is TRUE. — process_ocean_argo","title":"Process flagged ARGO data. Outputs processed\ndata to the output_folder along with plots if plot_figures is TRUE. — process_ocean_argo","text":"Process flagged ARGO data. Outputs processed data output_folder along plots plot_figures TRUE.","code":""},{"path":"/reference/process_ocean_argo.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process flagged ARGO data. Outputs processed\ndata to the output_folder along with plots if plot_figures is TRUE. — process_ocean_argo","text":"","code":"process_ocean_argo(   input_filepath,   output_folder,   plot_figures = TRUE,   start_year = 2005,   end_year = 2016 )"},{"path":"/reference/process_ocean_argo.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process flagged ARGO data. Outputs processed\ndata to the output_folder along with plots if plot_figures is TRUE. — process_ocean_argo","text":"input_filepath Input filepath output_folder Output folder plot_figures TRUE plot figures output_folder start_year First year process end_year Last year process","code":""},{"path":"/reference/process_ocean_argo.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process flagged ARGO data. Outputs processed\ndata to the output_folder along with plots if plot_figures is TRUE. — process_ocean_argo","text":"str: Output filepath","code":""},{"path":"/reference/require_package.html","id":null,"dir":"Reference","previous_headings":"","what":"Checks if a package is installed. If it is it's loaded,\notherwise stop is called with a message telling the user\nthe package is required. — require_package","title":"Checks if a package is installed. If it is it's loaded,\notherwise stop is called with a message telling the user\nthe package is required. — require_package","text":"Checks package installed. loaded, otherwise stop called message telling user package required.","code":""},{"path":"/reference/require_package.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Checks if a package is installed. If it is it's loaded,\notherwise stop is called with a message telling the user\nthe package is required. — require_package","text":"","code":"require_package(pkg_name)"},{"path":"/reference/require_package.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Checks if a package is installed. If it is it's loaded,\notherwise stop is called with a message telling the user\nthe package is required. — require_package","text":"name Package name","code":""},{"path":"/reference/retrieve_tutorial_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve the tutorial datasets and unpack them\nto a folder in th user's home directory or a folder\nspecified in their use config file. — retrieve_tutorial_data","title":"Retrieve the tutorial datasets and unpack them\nto a folder in th user's home directory or a folder\nspecified in their use config file. — retrieve_tutorial_data","text":"Retrieve tutorial datasets unpack folder th user's home directory folder specified use config file.","code":""},{"path":"/reference/retrieve_tutorial_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve the tutorial datasets and unpack them\nto a folder in th user's home directory or a folder\nspecified in their use config file. — retrieve_tutorial_data","text":"","code":"retrieve_tutorial_data(tarball_path, data_store = NULL)"},{"path":"/reference/retrieve_tutorial_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve the tutorial datasets and unpack them\nto a folder in th user's home directory or a folder\nspecified in their use config file. — retrieve_tutorial_data","text":"tarball_path Path tutorial data tarball (name changed) data_store Folder store extracted tutorial data","code":""},{"path":"/reference/run_bhm_gic.html","id":null,"dir":"Reference","previous_headings":"","what":"Run the BHM for Glacier and Icesheet data. Data is written to\noutput_folder. — run_bhm_gic","title":"Run the BHM for Glacier and Icesheet data. Data is written to\noutput_folder. — run_bhm_gic","text":"Note: accepts GRACE area 1 (mascon) data","code":""},{"path":"/reference/run_bhm_gic.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run the BHM for Glacier and Icesheet data. Data is written to\noutput_folder. — run_bhm_gic","text":"","code":"run_bhm_gic(   mesh_filepath,   data_filepath,   grace_data_filepath,   GRACE_dc,   output_folder,   use_inlabru = FALSE )"},{"path":"/reference/run_bhm_gic.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run the BHM for Glacier and Icesheet data. Data is written to\noutput_folder. — run_bhm_gic","text":"mesh_filepath Mesh filepath data_filepath GIC annual rates filepath grace_data_filepath GRACE data filepath GRACE_dc GRACE data centre JPL GSFC, see documentation ... output_folder Output folder use_inlabru Use inlabru instead INLA","code":""},{"path":"/reference/run_bhm_gic.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run the BHM for Glacier and Icesheet data. Data is written to\noutput_folder. — run_bhm_gic","text":"list: List output filepaths","code":""},{"path":"/reference/run_bhm_gic_bru.html","id":null,"dir":"Reference","previous_headings":"","what":"Run the BHM for Glacier and Icesheet data. Data is written to\noutput_folder. — run_bhm_gic_bru","title":"Run the BHM for Glacier and Icesheet data. Data is written to\noutput_folder. — run_bhm_gic_bru","text":"Note: accepts GRACE area 1 (mascon) data","code":""},{"path":"/reference/run_bhm_gic_bru.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run the BHM for Glacier and Icesheet data. Data is written to\noutput_folder. — run_bhm_gic_bru","text":"","code":"run_bhm_gic_bru(   mesh_filepath,   data_filepath,   grace_data_filepath,   GRACE_dc,   output_folder,   use_inlabru = TRUE )"},{"path":"/reference/run_bhm_gic_bru.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run the BHM for Glacier and Icesheet data. Data is written to\noutput_folder. — run_bhm_gic_bru","text":"mesh_filepath Mesh filepath data_filepath GIC annual rates filepath grace_data_filepath GRACE data filepath GRACE_dc GRACE data centre JPL GSFC, see documentation ... output_folder Output folder use_inlabru Use inlabru instead INLA","code":""},{"path":"/reference/run_bhm_gic_bru.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run the BHM for Glacier and Icesheet data. Data is written to\noutput_folder. — run_bhm_gic_bru","text":"list: List output filepaths","code":""},{"path":"/reference/run_bhm_ocean.html","id":null,"dir":"Reference","previous_headings":"","what":"Run the BHM for oceans. — run_bhm_ocean","title":"Run the BHM for oceans. — run_bhm_ocean","text":"Spatial ranges (range degrees lon/lat) process","code":""},{"path":"/reference/run_bhm_ocean.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run the BHM for oceans. — run_bhm_ocean","text":"","code":"run_bhm_ocean(   mesh_filepath,   grace_data_filepath,   altimetry_filepath,   argo_data_fileapth,   output_folder,   GRACE_dc = \"JPL\",   prior_range_steric = 25,   prior_sigma_steric = 30,   prior_range_manometric = 65,   prior_sigma_manometric = 10 )"},{"path":"/reference/run_bhm_ocean.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run the BHM for oceans. — run_bhm_ocean","text":"mesh_filepath Ocean mesh filepath grace_data_filepath GRACE data filepath altimetry_filepath Altimetry data filepath argo_data_fileapth Argo data filepath output_folder Output folder GRACE_dc GRACE data centre, JPL GSFC prior_range_steric Prior range - ocean steric prior_sigma_steric Prior sigma - ocean steric prior_range_manometric Prior range - ocean manometric prior_sigma_manometric Prior sigma - ocean manometric","code":""},{"path":"/reference/run_bhm_ocean.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run the BHM for oceans. — run_bhm_ocean","text":"list: List output filepaths : ocean mass, ocean steric process model output","code":""},{"path":"/reference/run_ocean_model_bru.html","id":null,"dir":"Reference","previous_headings":"","what":"Run the model using inlabru — run_ocean_model_bru","title":"Run the model using inlabru — run_ocean_model_bru","text":"Run model using inlabru","code":""},{"path":"/reference/run_ocean_model_bru.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run the model using inlabru — run_ocean_model_bru","text":"","code":"run_ocean_model_bru(   ocean_mesh,   combined_data,   grace_se_rate,   altimetry_se_rate,   argo_se_rate,   prior_range_steric,   prior_sigma_steric,   prior_range_manometric,   prior_sigma_manometric )"},{"path":"/reference/run_ocean_model_bru.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run the model using inlabru — run_ocean_model_bru","text":"ocean_mesh Ocean mesh object combined_data Combined data object, contains grace points, altimetry argo data grace_se_rate GRACE SE rate altimetry_se_rate Altimetry SE rate argo_se_rate Argo SE rate prior_range_steric Prior range - ocean steric prior_sigma_steric Prior sigma - ocean steric prior_range_manometric Prior range - ocean manometric prior_sigma_manometric Prior sigma - ocean manometric","code":""},{"path":"/reference/run_ocean_model_bru.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run the model using inlabru — run_ocean_model_bru","text":"model output","code":""}]
